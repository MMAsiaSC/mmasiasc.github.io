<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[ACM Multimedia Asia 2021]]></title><description><![CDATA[ACM Multimedia Asia Conference (ACM MM Asia) 2021 will be held from 1 to 3 December, 2021 in Gold Coast, Australia. It will be held in a hybrid mode by offering both online and offline events. A live in-person conference with virtual online component will be enabled.]]></description><link>https://www.acmmmasia.org/2021</link><generator>GatsbyJS</generator><lastBuildDate>Wed, 08 Dec 2021 09:05:03 GMT</lastBuildDate><item><title><![CDATA[Awards]]></title><description><![CDATA[ACM Multimedia Asia 2021 Best Paper Award Language Based Image Quality Assessment Lorenzo Seidenari (University of Florence) Leonardo…]]></description><link>https://www.acmmmasia.org/2021/awards/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/awards/</guid><pubDate>Fri, 03 Dec 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;ACM Multimedia Asia 2021 Best Paper Award&lt;/h2&gt;
&lt;h3&gt;Language Based Image Quality Assessment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Lorenzo Seidenari (University of Florence)&lt;/li&gt;
&lt;li&gt;Leonardo Galteri (University of Florence)&lt;/li&gt;
&lt;li&gt;Pietro Bongini (University of Florence)&lt;/li&gt;
&lt;li&gt;Marco Bertini (University of Florence)&lt;/li&gt;
&lt;li&gt;Alberto Del Bimbo (University of Florence)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Message from the authors -&lt;/strong&gt; &lt;br&gt;
“Thank you for awarding us the best paper prize! It was an honor to present at ACM MMAsia. The work is the result of a coral effort from all the authors in the last year. So my thanks goes to the Award Committee and to my coauthors!”&lt;/p&gt;
&lt;br/&gt;
&lt;h2&gt;ACM Multimedia Asia 2021 Honourable Mention&lt;/h2&gt;
&lt;h3&gt;Towards Discriminative Visual Search via&lt;br/&gt;Semantically Cycle-consistent Hashing Networks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Zheng Zhang (Harbin Institute of Technology, Shenzhen)&lt;/li&gt;
&lt;li&gt;Jianning Wang (Harbin Institute of Technology, Shenzhen)&lt;/li&gt;
&lt;li&gt;Guangming Lu (Harbin Institute of Technology, Shenzhen)&lt;/li&gt;
&lt;/ul&gt;
&lt;br/&gt;
&lt;h2&gt;PhD Lightning Talks Competition&lt;/h2&gt;
&lt;h3&gt;Winner&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning to Diversify for Single Domain Generalization&lt;/strong&gt;, Zijian Wang&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Highly Commended&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A Framework of Noisy-Label Learning by Semi-Supervised Learning&lt;/strong&gt;, Zhuowei Wang&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CausalRec: Causal Inference for Visual Debiasing in Visually-Aware Recommendation&lt;/strong&gt;, Ruihong Qiu&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;People’s Choice Award&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Why Temporality is important for Knowledge Graph?&lt;/strong&gt;, Jiasheng Zhang&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Map of Virtual Rooms in Gather.Town]]></title><description><![CDATA[Welcome to ACM Multimedia Asia Gather.Town! Main Entry When you enter the town, you are supposed to be sitting here. Exhibition Hall The…]]></description><link>https://www.acmmmasia.org/2021/gather-town-map/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/gather-town-map/</guid><pubDate>Mon, 29 Nov 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Welcome to ACM Multimedia Asia Gather.Town!&lt;/p&gt;
&lt;h2&gt;Main Entry&lt;/h2&gt;
&lt;p&gt;When you enter the town, you are supposed to be sitting here.&lt;/p&gt;
&lt;div style=&quot;max-width: 800px;&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 855px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/2fc5263015f99b187206e30668912c7b/4b293/Overall-Map.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 104%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAYAAABG1c6oAAAACXBIWXMAABYlAAAWJQFJUiTwAAAENUlEQVQ4y4WV2W7bVhCG9T69SYOiLRKJPNy0cJFkxo4scZEpmZJsy5YoUZbteIuTFHXitDFQICl6USCvUfTZvoKUHTe96cWPIUjMx1nOzCnI8gwhz5CllVW0KUKdfLGakWDaS0z7EKMyR9UfvqvG9M53vrLyjMIDcI5QktypVj1G11NUdY5lndBqvaLdfkOtdkyxOKFcPkRV0xVY+Q9Q3AGFNEeWp9QbJwTeNdXKMb3ohkH8AV09ZGd0Szd8R9lYMhx8YG/nN0xriRBJ7nvPKUgZSEqRpBQhZkhiQrE0RRZTKpVjLPsYRZljO6e46+fUaifYzgn1xhnl6gJZJF9HqGdhy2M0sY8mJrRbCw4vXvD60ytCb8bVx0uSoxNG+ylv/rgi9BLOby9YXJ7itef5z+7rn0e47uwS917RCy9wKjv43oJONKB3MKZWDejt77PeGtB0+8TJlFolIBrvEW4P2YlTNDW9A66iLNjVhKh3SxD+glOd8Pv1MaOoz/c/tqnUAuqtTZxWi/p6C+fZBqYTUCx2EGqHfj9FUVKKxYRSMSErX6HzbEJr/TRXxx3z57uUyTDiacmn7nqs7UpYo2+x9x7RmH2Pu9lCVkIsKyAdL3GMhEY1pWkusPQphbpziO9f0elcYppLhvu3NJ9PkKQOzppPs/8Ec/QIa/cxdvcH3M1NnpR8Rp2Yv2+vCOwDOu03DOP31I0JBaN6zjP3imbjEqN6yfLiM150QrG0uQKOH1PbeURt8B3O+BuarRZChBgVj1GcYusTSqUZQiyoiAMKdvMK37/OVTFfEgUJtu0hCR+7HrCxbbMW1nADi/XtKs11H6EEGBWfYZzSqCT0eu/p9d9Tr2TnUF4gySmyyIq7YCtMMa3MyUfTAwyji1HuYlS6GMZW/k7R/Bw4ugNGWzfE8Qea1TkF2zrGbZ7j2CeY1SPOXtzgB2MkuYOaO3sI4a+keai6/wWYHRvHmKIqCxRlQVU5oOCuXRCGbwn8axqNc27efiIeLChK7Ryo6wFmM6Dm+hhGmMMyaWWP7f4cSzvA934mim7yjhc2Wy+Jtq7p99+SwfdGp6y5PWTFyx1Ncws31mkOZOxGgKL6CNXHsULOJsu8s89bPxFu3WDqSRbhGV7nNc83LnHdMz59/Mz+wQuK0mYOrJkRa5HBxkDJgaoa8FR0GLRj/vr1iq36BFU6oCwmNPR9CqqSkmk1QilRd4FlB3kUWb2ytHt+m3rDRzMC1LuU9bJHrzfHUGfoYoqRrT6RUMjm8F6l4py4t8Spd5EV/64pWXoequbl6eZA1b87Nos8GKk0zzdOPnqroV4Ndg7sL3EaWYQempF11cufM6vpK5t1vlz1GA0WaEr69ba539gZtFScEXWP2NudM9ieMYxXGg0enu+1M5wzjI9QxMP18eUK+Pe9oog5hrb4f+lZul/vwgz4D2HvVro43Y1WAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Overall Map&quot; title=&quot;Overall Map&quot; src=&quot;/2021/static/2fc5263015f99b187206e30668912c7b/b331b/Overall-Map.png&quot; srcset=&quot;/2021/static/2fc5263015f99b187206e30668912c7b/56d15/Overall-Map.png 200w,
/2021/static/2fc5263015f99b187206e30668912c7b/f46e7/Overall-Map.png 340w,
/2021/static/2fc5263015f99b187206e30668912c7b/407f2/Overall-Map.png 520w,
/2021/static/2fc5263015f99b187206e30668912c7b/b331b/Overall-Map.png 855w,
/2021/static/2fc5263015f99b187206e30668912c7b/f3cf6/Overall-Map.png 890w,
/2021/static/2fc5263015f99b187206e30668912c7b/4b293/Overall-Map.png 1606w&quot; sizes=&quot;(max-width: 855px) 100vw, 855px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/div&gt;
&lt;h2&gt;Exhibition Hall&lt;/h2&gt;
&lt;p&gt;The tracks of short paper, demo, brave new ideas as well as three workshops will be hosted here. &lt;strong&gt;Presenters&lt;/strong&gt;, please stand on the stage and Session Chairs please sit at the front-right as indicated in the figure below. Audience can freely choose a seat anywhere else.
We will also be broadcasting all the events on Webinar in the Exhibition Hall simultaneously. Feel free to watch other talks in this room if you prefer.&lt;/p&gt;
&lt;div style=&quot;max-width: 800px;&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 855px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/c459f78118fbbd40482c0bf8240e9643/6e301/Exhibition-Hall.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 88.5%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAABYlAAAWJQFJUiTwAAACvUlEQVQ4y41Ta1faQBDlD1EPikpSMY/dDeFtqFJKNgkoEFHP8XFU5NWe/vLbs5MQgX5oP8yZ3czk7tw7M7nVfAVHXKNYvIEwPIxLB7jTC7jVjzBN/aBUxFgvIs6+HVLO+PQLXKNN/zI2xHK2RG71sUbFjVDSInCrC0+30dPLCEtF9LSvmGhHmJQKGJcKdO6VdIRaET39jHKFdQVNH8CpRFgowPnbElE0JVCLSRgsgmlL2HYf/NzD6LSAJ14mU8CifAHL7sO0fcq1eQDL9tFoBpi/L5Fbzn7i4e4Z9foNJTEuwZiEKQaonF9gcpLHr7qG3w0N8UkebrkFkw8ox2Y+LFvizJCoNwIsFGDVbcOwOuAigk1JqfEQzOyirXN0DBffDJfOzLhKYjwAExG8Rohuw0ejFSUVHh+f4VRrQTgDAlKJypt2n+iYIiJqJIVIKJpWH8KJcOFN0fHGuPQm6FyOMH9bIPf+Msc0fiTKSgsmJBxXJccQTgDGfXAlA5kP7gRoe3Giud2HxXyUDf+T8mK2xvXwDhV3QE1RgNwJUa0NwUVA93Or/xkTAdzaEMIJs7tpSdQ2gMvZmjpkWGlDRFKNElx5i0t02t/hCCVBElex7Ey5AbzOcNPlNZqtkF5R+iVVJZ7OPMDVZQynEtID2XflnYAo12rXuJ8+Y/Y63wf8rFB5K61SVW+nlLN4Vq2kBgUyxodqyn6FGWUh4ToeNYI6z7cAt40ruSRqdZlq+LHRcBdQmdIt02kfcIuNKqbRDNMuv69IUCXsPmU1c4xvAfK/KW8A682sy2r1XrI55NuvC/nfFWZzeH/7iP6PUTaHm9n7J+AWsNrnZjtMxiafP8bBYTXZZb5FZ8eCjP6OqUmwfVSrQ8STR8xeF8j1ugE8L4JbHdLAqtnitAFqYxJfNrq0ZjuxNK5Yue4A49EDAf4BWhW5t7FGIRcAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Exhibition-Hall&quot; title=&quot;Exhibition-Hall&quot; src=&quot;/2021/static/c459f78118fbbd40482c0bf8240e9643/b331b/Exhibition-Hall.png&quot; srcset=&quot;/2021/static/c459f78118fbbd40482c0bf8240e9643/56d15/Exhibition-Hall.png 200w,
/2021/static/c459f78118fbbd40482c0bf8240e9643/f46e7/Exhibition-Hall.png 340w,
/2021/static/c459f78118fbbd40482c0bf8240e9643/407f2/Exhibition-Hall.png 520w,
/2021/static/c459f78118fbbd40482c0bf8240e9643/b331b/Exhibition-Hall.png 855w,
/2021/static/c459f78118fbbd40482c0bf8240e9643/f3cf6/Exhibition-Hall.png 890w,
/2021/static/c459f78118fbbd40482c0bf8240e9643/6e301/Exhibition-Hall.png 2996w&quot; sizes=&quot;(max-width: 855px) 100vw, 855px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/div&gt;
&lt;h2&gt;Women in Multimedia Roundtable&lt;/h2&gt;
&lt;p&gt;This room is to host the special session – Women in Multimedia Roundtable.
&lt;strong&gt;Panellists&lt;/strong&gt;, please sit around the table and all other attendees can sit on other seats.&lt;/p&gt;
&lt;div style=&quot;max-width: 800px;&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 855px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/65a396fc2315dc76b393927074a868bf/20dd7/Women-in-MM.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 64%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAAC0ElEQVQ4y4WRW1PbVhSF9Wuay5BYd0sivoMpFBnfAGNsy7ak+gZO0gwwpmkGHDCQkk47Ux6bhD6S5Pf053wdnUDatzysWWufs86evc6WJuMe19M+H6YDrl8PBN/hr6M+748HvDvu8+5Wf6kHvP+f78PrIdfHfV497yHttOpMa3ne+AUuui5vfJfzrsuvQYHfggKXQYGr3TJXuyX+3Cny9scCV0OXy9DlvPPFf+EXONleYrexidQqVwnnDYF+Mo7v6PQSJjU5hmcobBka9XSGZj5FIzOPZ2r8Us3TS9n0Ega+rTNMx6nFYnjFElKnXOFsI82rYorzrRyHbpLZZpaWJhM4Gl1Lw7c1AkvFt3R8SyNM2LTjOhf1BSarCS6beQJTxi8VkYL1KqGlEjg6oaMT2JrgjqnSjas0tRg7WYdZ0xUJPE2mayp4usxRJcNPeYfjcgZPV3hZTH27YTTh85zF2+0co5RJ19LFWUOTaZuKmHi0qNO1VYZJA6ldrnC6nublWoqzrZyIcLqRpanGROPxcpa/+wv883uZPxoZnq1kaesyg+V5+os2niozrqmEKZW6qiL51Sq7KYNRKs44azFImoKjhUQTBvMmT7Mms2pCTBA+iWLH2FnK0cskaKkxOnGZjqkwThtI4ca6MLRNlbahiKiC4yp+XKVjKISOwSjnENi6qKOYTeUxh6tPeLZgi3QdQ2ZUWkHqVioclZJMVpNMqxn2VxIcldO0dAXPVGgaMh1LpetotG7r6A89Q+WsvsjBDwkuGnn8aMuVMlKzXOVF3uJpxmT/e1vE38tb1B/PUZm7T/HhPapzDyg++E7o0sN7NJRHNORH7C3ZjJI6B8sODXkOr1RCOhy22dt0Odh02b/lCJOtAj/X1wQOI95e+1pHdxGEv3bHLpOeh/Rx2uHzLOTjacDnWcCn0+CrvjnxhRa41Tcn/3k+3SF6fxJwM+3wLxgzD5fkpl1nAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Women-in-MM&quot; title=&quot;Women-in-MM&quot; src=&quot;/2021/static/65a396fc2315dc76b393927074a868bf/b331b/Women-in-MM.png&quot; srcset=&quot;/2021/static/65a396fc2315dc76b393927074a868bf/56d15/Women-in-MM.png 200w,
/2021/static/65a396fc2315dc76b393927074a868bf/f46e7/Women-in-MM.png 340w,
/2021/static/65a396fc2315dc76b393927074a868bf/407f2/Women-in-MM.png 520w,
/2021/static/65a396fc2315dc76b393927074a868bf/b331b/Women-in-MM.png 855w,
/2021/static/65a396fc2315dc76b393927074a868bf/f3cf6/Women-in-MM.png 890w,
/2021/static/65a396fc2315dc76b393927074a868bf/20dd7/Women-in-MM.png 2590w&quot; sizes=&quot;(max-width: 855px) 100vw, 855px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/div&gt;
&lt;h2&gt;Poster Rooms&lt;/h2&gt;
&lt;p&gt;A number of rooms are set up for displaying posters. &lt;strong&gt;Authors&lt;/strong&gt;, please stand in front your poster and communicate with visitors.&lt;/p&gt;
&lt;div style=&quot;max-width: 800px;&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 855px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/0a0acc966cce9adc6b5624ff158a837a/d558f/Poster-Room-1.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 65.49999999999999%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAAC1ElEQVQ4y0WT6XLiVhCF/UYT29gsQoAkdqOFVUKAEGLHMmI14GWSSs1MzVt/KYkk/nHq1Onb3VV9uu/N5fjOxt/jDabMRksm7pzgecfEmTLzFkyGM9bLDdPRIoLnTPHnaxZTn/FwzsxbMh8/s9++cj68cfO6O2NYQ8yZT9V0seY+NWtM3dtQangYXsCTOcMcPaN1p7ScJQ1nSbPr0bBG6B0X05lSqnTZb47cHLdnqi2X/vaDcndKP3iLCnvBOxVrTts/Ux+s6C4PaEOf9mxPexxgTgKa4wBj+Iy52CHlu+yCIzenwxuybHL3rYKQanL/R5Ws0OLx7gkh1SB2G+om8fsaybjBQ0wlFTdIxXUeH3QS8TqxOxWl0Ls2fN1fKJZ6iJk2smKTybbJyTZitkNOCrVJVrJJ50Lukgs5Z5EN30OEOtOOGm7DhsfNCc2e4P/5C91b4n/+xHBXTC6/0ZwX3NNPGqMXJh/vmC97vM0HA/+Eu9zhrvbY04DZ9oxU6HLcncOlXCg89VDqI3LVPorhUq4NKDZGSNU+WW1I2+qjtfrk1QGS6lDSXUr6EElzkFUHSXeoqB7eaHH1UJI63H4rk0rWub+tIgpNYreVSMfuqmSEVuRd/FHnIXb1MvGofcXuazxpc5JJ5ephqWqjlCzkokmx2iNfccgVbTJyB7nUQy4NKJR75PImSsEiX+6TL3aR5Bay0iZfCOscNuvD1cOaPWH59y9q3orFXz8wvBXj779R3TXO+w+a0y2T83c6qwPu4ZNBcGL4ssNZH7AWG8bH0EOb/eb1OrKSt4gnjWjT8ZSBIpkI6TrpTJuE8K8Wm6TSTRJCg2y2FV1DUmwiZFok0g2Ugv3fHV4oFG1SqTDJQhDaiJkvTosdBLFDJm0gyTZp0YxiEdIdRNGMcv+/w9P+QqM5oKY5qHqI4RdrDpoxxA8+UfURVbWP0RhF8WvOV41u9KOv9w/EYwmJWYPWKAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Poster-Room-1&quot; title=&quot;Poster-Room-1&quot; src=&quot;/2021/static/0a0acc966cce9adc6b5624ff158a837a/b331b/Poster-Room-1.png&quot; srcset=&quot;/2021/static/0a0acc966cce9adc6b5624ff158a837a/56d15/Poster-Room-1.png 200w,
/2021/static/0a0acc966cce9adc6b5624ff158a837a/f46e7/Poster-Room-1.png 340w,
/2021/static/0a0acc966cce9adc6b5624ff158a837a/407f2/Poster-Room-1.png 520w,
/2021/static/0a0acc966cce9adc6b5624ff158a837a/b331b/Poster-Room-1.png 855w,
/2021/static/0a0acc966cce9adc6b5624ff158a837a/f3cf6/Poster-Room-1.png 890w,
/2021/static/0a0acc966cce9adc6b5624ff158a837a/d558f/Poster-Room-1.png 4058w&quot; sizes=&quot;(max-width: 855px) 100vw, 855px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/div&gt;
&lt;h2&gt;Lounge Room&lt;/h2&gt;
&lt;p&gt;Feel free to chat with each other in the lounge room.&lt;/p&gt;
&lt;div style=&quot;max-width: 800px;&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 855px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/f1e500c377d5d8c9965fff9f9256e863/9ad8b/Lounge-Room.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 89.99999999999999%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAABYlAAAWJQFJUiTwAAADrElEQVQ4y12UW1caSRSF/S/GGwhNd1dfqhpEgoMocm3iPSICAt5mMjGJgJLgZF7nL3+zutCJax72qlVdq3ftc/Y+tfBwP2E6njF9nDGb/s3N1T375QPubr7wY/qTp8mM6eSZpxdMxn8xGc00Hr8+8fjwnfH4mcfRjNGXCQtP4x+cn15Q2Q45Kjc5qzcZ1Oq0Gk0Oy00OKh84KDc5qYac1UJ6YYPf92vchFX+rBa5blYZHu4zbLUYf5uyEClrFOsUVkz21m2qKZ8te5u66ZF9Z7GXdMgtC3ZjFqFhcukmufRTdF2DC8+iKy3aVpxWcZvRw3cWppMZzVKT/JpH2VSUky4bSw61lEMl5bNrSEqGT2i5HDoul8rmj7zP54LkOutwm/O5zQquK4W5wqhHp5UGO2sWH2yXYWAxCGz9Y0/atD2brpzvByo6sxhmHO5yHrebLnc5n7us4FYTPrEQNXQQ1riRCbpScGg7nDiCvrI4cwX5uM9W3OdYCI0jIehJ6w1sem6Si9LOvORHrTDkzDa0wvy6orDua8KWFxFKfluXuhWFhNT7iCSqoq9s+lE1nkGnVOThteTIxUPT5FC4bKxJTXjuzRXVUi5HwqFpOeRikvcxSTsywzd1Gy4DQc816JZLTJ9+zgmPKyFHZkoftj2hFVQNl1xMUUl5XKXnPexIwYVnMywEDPOSS2nRDwQDadLaTNM6brEwGc/oNeo6DsO0rU25SZs0TJeNVUkxLui4hkbXS9ERCa7zGYY5RcdJ6G+DtODUjJEx7TnhZVin4yQ5clwObEcbUzcEJzLNp70io+o2k7DEqFbU6+fdLe5LW3yr7nC/V6DvG5wX8txef4pcntEP67RsA7Ws2IgpcnGF907y0XPppBUVO0NdpKnZaUI3w1VWcr05x3BTalO65V0eJ8//I1wJOM9IbvIStRpwbAut1FmUFK0Ncutp/EWPjmfR801tTDQpry6PIpd/ESYJVgMqIqDpBahVpV1umg6ZmOKfrxMGtSPkoqCnxNxhZdGLVveFMMrhW4WZFYVcfsGS1AEPTQdvSZE3smzEMwTLUk9OZF4U8mg09+I2F6+Ery53nSRtX+hRi3Du2TrckUm5NZ98zCO/5lOIe5rwKm1p9ZvxgNyKQ3t3h/FbwrYwKBsee0lPr6WEx0dX/DfXAzW/oK/sl3Jt3ZIop7WE9Wv0Xks+swz8JUU2rnifULiLin3L0Y9BNLMt3+FCOjrM0X434bG5JvUr1EhanL8o/BdFEKlkcFc4/AAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Lounge-Room&quot; title=&quot;Lounge-Room&quot; src=&quot;/2021/static/f1e500c377d5d8c9965fff9f9256e863/b331b/Lounge-Room.png&quot; srcset=&quot;/2021/static/f1e500c377d5d8c9965fff9f9256e863/56d15/Lounge-Room.png 200w,
/2021/static/f1e500c377d5d8c9965fff9f9256e863/f46e7/Lounge-Room.png 340w,
/2021/static/f1e500c377d5d8c9965fff9f9256e863/407f2/Lounge-Room.png 520w,
/2021/static/f1e500c377d5d8c9965fff9f9256e863/b331b/Lounge-Room.png 855w,
/2021/static/f1e500c377d5d8c9965fff9f9256e863/f3cf6/Lounge-Room.png 890w,
/2021/static/f1e500c377d5d8c9965fff9f9256e863/9ad8b/Lounge-Room.png 2940w&quot; sizes=&quot;(max-width: 855px) 100vw, 855px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/div&gt;
&lt;h2&gt;Chat Room&lt;/h2&gt;
&lt;p&gt;If you would like to have a small group discussion with other attendees, feel free to use a chat room as the meeting room.&lt;/p&gt;
&lt;div style=&quot;max-width: 800px;&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 855px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/f95209cd8889d84da43dd1c96f4e9e38/77666/Chart-Room.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 100.49999999999999%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAABYlAAAWJQFJUiTwAAADVUlEQVQ4y52UR3PbVhRGtfYi2vgfWGOLIsHeicICNrCTklUsEuwVAFW8kNfZZBLFsuKSzPjHngxAyU5iKZ5k8c27eMA7892He+9W77RPvztgpI8Z9EaMB1POzHNWcwNrtWYxXTEZzrg4e81ytnKeV3PTie1v7TND3daYk+NTtga9IYvxgtXMZDU1uLAueX/znptfbvj02yc+3n7kzes3fP79M29/fsv1T9d8ePeB219vubq8YjlZsZqaGDMT/VTfAI2ZQac1QpH7ZNMjZKlPWh6gyAOymTEZe0/USSvDL8ooQ+edHZeKfcy5Rb/b3wCtuUGtMsXlaiG4y3iFCoJH28i9Wb2ev+x5NNx7JQR3kd0XGqKoYy3W6PdAc25Qr84Q3E0C4SyBaJ5AVCUQUfGHVfyRr7L3vDEVXyqL15/A68uiSP8KzBGIFvFHCoRiRUSlRlzUSEiVjUQNb6hALJHn3ZWKEi+TSg1ZL6xvgZ4vwALekEokXiSr1kjKZaS0hqhoyJkqe740mcIxf1z/SFrcJ5XUsebWww794RzBWJG4VCWRKlOvvqReO6RWfUmrcYySrhGOF0il22itCT5fC0UacWFcPJ6yDRQzDfL5JmfLNevF2nFwaV7Qbh47qUcSZdxeCUEoEIu2OeyccHL06mGHoViJpFwjpzYwpgbLicFisnLAjfoR0WSJaKpCIFIgGCuw8yLA9g9PqVebjztMSFXUfMuBmPM15sxy0mo3T4gmy45DfzhPMFZiZzfC9vZTGg8B7x0quRbpTJWR3Vq9MYPuiOlgilZuk5QrxFIVfGGVULzEs+dBnjzZftyh/ZfjYgVfKIPXL+ELyHjvJARkoknNycIfyeMLquy6Euw826PV6DwOdMomqZHOdUgpDcR0EynTciQEc5v7i6oIQhkppTtX0nulPwRUncK2ofZqpx+8031sw+6BdpvK0hDLLuzTb8qmQSCUIRhRCUZsF7mN438oeNeCwUgWr6ChyMOvhd2/Gw5VbYLL1UZw1/C4G3jc9e9KcNfZczURU72/97I9Bw8PJpSLIyrlKZXy5G79vrTSmEZtuJmH3QFbI33IcrJgvTQ5NyzOV+Z/lMXZ0sScm/S7OlsH+132O6cc7PfY73T/t+zzzeYRfwKXJwGNSGgJJAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Chart-Room&quot; title=&quot;Chart-Room&quot; src=&quot;/2021/static/f95209cd8889d84da43dd1c96f4e9e38/b331b/Chart-Room.png&quot; srcset=&quot;/2021/static/f95209cd8889d84da43dd1c96f4e9e38/56d15/Chart-Room.png 200w,
/2021/static/f95209cd8889d84da43dd1c96f4e9e38/f46e7/Chart-Room.png 340w,
/2021/static/f95209cd8889d84da43dd1c96f4e9e38/407f2/Chart-Room.png 520w,
/2021/static/f95209cd8889d84da43dd1c96f4e9e38/b331b/Chart-Room.png 855w,
/2021/static/f95209cd8889d84da43dd1c96f4e9e38/f3cf6/Chart-Room.png 890w,
/2021/static/f95209cd8889d84da43dd1c96f4e9e38/77666/Chart-Room.png 1170w&quot; sizes=&quot;(max-width: 855px) 100vw, 855px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Q&A Guidelines]]></title><description><![CDATA[For audience Pre-recorded presentations of all accepted papers will be available in the program during the conference period. Audience are…]]></description><link>https://www.acmmmasia.org/2021/q-and-a-guidelines/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/q-and-a-guidelines/</guid><pubDate>Sun, 28 Nov 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;For audience&lt;/h2&gt;
&lt;p&gt;Pre-recorded presentations of all accepted papers will be available in the program during the conference period. Audience are welcome to ask questions in running sessions or when joining the session of “Social Connections on Gather.Town”.&lt;/p&gt;
&lt;p&gt;Map of Virtual Rooms on Gather.Town can &lt;strong&gt;&lt;a href=&quot;./gather-town-map&quot;&gt;&lt;em&gt;be found here&lt;/em&gt;&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;For presenters&lt;/h2&gt;
&lt;h3&gt;Regular, Grand Challenge, Applied Research Tracks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Authors are required to attend your scheduled sessions on &lt;strong&gt;Webinar&lt;/strong&gt; for live Q&amp;#x26;As right after each presentation.&lt;/li&gt;
&lt;li&gt;Authors are required to attend “Social Connections on Gather.Town” session in &lt;strong&gt;Gather.Town&lt;/strong&gt; on Day 2 for live Q&amp;#x26;As. Please find your poster, stand in front of it, and wait for others to come over for discussions in the Poster Rooms (see the example below). You are suggested to use “Author of $modelname: $yourname” as the ID for being recognised by other attendees.&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&quot;max-width: 600px;&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 855px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/fb58206a5e99edca9aeab3e5fc3ae23f/c7b1b/ray-at-poster-room.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 52.5%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAACxLAAAsSwGlPZapAAACdklEQVQoz22R2W7TUBRF8yUIgZB4QUK0ieMp8ZCpTew4gzPYTd0E0pgEp+lIW9rSJ/rAX1Ah4K/yI9ZCcYAixMPV0T17n3Wu7k4VijZb2yO0qofS9sjZfUzTJ1/rI7c9dMunXgnQHB+tt5v4DNNDtrqIdRe94mHVJ3GpbNHvhsvUs+cZBqNrVKVDZsNCEpsYhQGy1E7usuxSKg6Qsg0yr2qJb6WLgoOwYaEbPl5wGT9++oLqlr9M9b09SrU9FLlNJmMjSa01UGmTSdsoSht9BRBbbG7aKOoaKAhN0uk6qtqlVA3i3cGI8ehkmZpHRxQqAcXaGyxvRtnZRzd3KNfH1LwpJfsNhuHj9IZ0RmOqjT00w6fpj+gM96lYAaXtYXx6fMZ8drNMhWGEKrnY0yNuvnzD+3CLqnZwjy+5uv+Oe3zBttEj+HjD3Y97nNkCWXUJP3/i7us95WGIKnfiebQgml4vU2/DCM3wyOldRK2ForsY5g6K3iGrNckZPbaLQ2TNRci3UfVuoot5l2y+TU7vo5tefBAdPgDzhpf8hZC2kaUWZjFAUdx1SHKDRt1BEhukN2vkcl3M4m4SnpC2UHPdf4Fz9MJOEkI6U0WUHGS1lVRBalAtDwjcJnmzm3hWfTXfIZutI4oOmu5hFPz4YP4LGE5mGAUXJddBEG0kpUlWdsiuBo0eff8Ipxay1RijF/w/nlVdLc7rPmbBjQ+igzXw/PSc/fGUcDJn9drfdTKJiN4dUtlq8ujJS1pNn9l0kehh+OANJxGT/Vl8+f6C+fRqmTqaX3P4vxNdcXp4y64/Q9dsXu8tOFncJv2/fev5m3gRXREMzpY/AWQntPK9hCMJAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;at-poster-room&quot; title=&quot;at-poster-room&quot; src=&quot;/2021/static/fb58206a5e99edca9aeab3e5fc3ae23f/b331b/ray-at-poster-room.png&quot; srcset=&quot;/2021/static/fb58206a5e99edca9aeab3e5fc3ae23f/56d15/ray-at-poster-room.png 200w,
/2021/static/fb58206a5e99edca9aeab3e5fc3ae23f/f46e7/ray-at-poster-room.png 340w,
/2021/static/fb58206a5e99edca9aeab3e5fc3ae23f/407f2/ray-at-poster-room.png 520w,
/2021/static/fb58206a5e99edca9aeab3e5fc3ae23f/b331b/ray-at-poster-room.png 855w,
/2021/static/fb58206a5e99edca9aeab3e5fc3ae23f/f3cf6/ray-at-poster-room.png 890w,
/2021/static/fb58206a5e99edca9aeab3e5fc3ae23f/c7b1b/ray-at-poster-room.png 1632w&quot; sizes=&quot;(max-width: 855px) 100vw, 855px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/div&gt;
&lt;h3&gt;Short Paper, Demo, Brave New Ideas Tracks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Authors are required to attend your scheduled sessions (i.e., Lightning Talk Session 1 on Day 1 and Lightning Talk Session 2 on Day 2) in &lt;strong&gt;Gather.Town&lt;/strong&gt; (Exhibition Hall) and prepare to answer questions raised by the audience during playing your presentations. Please sit at front rows as ray did in the example below.&lt;/li&gt;
&lt;li&gt;Authors are required to attend “Social Connections on Gather.Town” session in &lt;strong&gt;Gather.Town&lt;/strong&gt; for live Q&amp;#x26;As. Please find your poster, stand in front of it, and wait for others to come over for discussions in the Poster Rooms. You are suggested to use “Author of $modelname: $yourname” as the ID for being recognised by other attendees.&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&quot;max-width: 600px;&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 855px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/0ae457b4ac4dc38866c128fe9bc834aa/b5cb9/ray-at-exibition-hall.png&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 71.50000000000001%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAACxLAAAsSwGlPZapAAACcElEQVQ4y4WSSVcaURBG/RVZZpFssogKzdgD9Nx2A7YoIEg3gwxq1GNiTuII5qffnH6IgFlkUaf7vfrq1ld13tbz/Zxm84xMZoBtdWkqKpGcpavk/httucCe2SSXH+L7pzzdP7M1f3yh1TwnnT7FNjoMC2mu5V0u5TRXcopvirQRyf0yNy3sEOgHZLJj9vZGzB5mbM0eXmi1JuSLMbYdUdN8jlSLriLTkMtM5CxTWWIqZziTJY7lEh1ZpqEY7KsertVGVvtUq8MF8PH3nMuLG8JwjGFFGM4Aw+lhOxGm3mJUzHKlSVxrEtNiGr8UYjk9TDsW2uQ/qTuoD0jWJ4A3V7fU61N0M8J2Yiw7xnT6WHqTUT7Fs5dnFhQ4L+xQKdUw3KHQiHB66GZMuATWKodIORfXG4iu5prQNE/wtCpBuUZVDwm0KpZxjOn0hMawYuq1mEO/Q3jQXwD3qw2KSoAfjATMdldhOjF+MKbijynbfUx3gOX23vKWE+O4EaYVrRw+3b3w88cvGo1zMbLjrQpMOyKsjKhXJxhWF8fdbCjW4y5GXu3wbs54dIkfLEZeBwoXr04TN8u795rESC1MRhbP5g/tzoSSfoLtbgpFsdsTsHWg/c/YfaLobOEweYcnJ1PKRlcIk+7O63d5XgJF7l0ka6lURlxefOf25m4JnKyAb85iXK+Ht9d/e0rrrhfuIlGjl4+Rtk3S2eomMBGadhdVbVFQGkj5A9LZfWSthaa3KapNSkZbaJbOHa+PorX48FEht117B/T6qKUGOSlgVwr4/MXg4yeVdK5GUT0ila1Q1A5RS81X4MKhVm7zNeVRUEL+AkIIF5UqCUiiAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;at-poster-room&quot; title=&quot;at-poster-room&quot; src=&quot;/2021/static/0ae457b4ac4dc38866c128fe9bc834aa/b331b/ray-at-exibition-hall.png&quot; srcset=&quot;/2021/static/0ae457b4ac4dc38866c128fe9bc834aa/56d15/ray-at-exibition-hall.png 200w,
/2021/static/0ae457b4ac4dc38866c128fe9bc834aa/f46e7/ray-at-exibition-hall.png 340w,
/2021/static/0ae457b4ac4dc38866c128fe9bc834aa/407f2/ray-at-exibition-hall.png 520w,
/2021/static/0ae457b4ac4dc38866c128fe9bc834aa/b331b/ray-at-exibition-hall.png 855w,
/2021/static/0ae457b4ac4dc38866c128fe9bc834aa/f3cf6/ray-at-exibition-hall.png 890w,
/2021/static/0ae457b4ac4dc38866c128fe9bc834aa/b5cb9/ray-at-exibition-hall.png 1532w&quot; sizes=&quot;(max-width: 855px) 100vw, 855px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
&lt;/div&gt;
&lt;h3&gt;Workshops&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Authors are required to attend your scheduled sessions in &lt;strong&gt;Gather.Town&lt;/strong&gt; for live Q&amp;#x26;As right after each presentation.&lt;/li&gt;
&lt;li&gt;Authors are required to attend “Social Connections on Gather.Town” session in &lt;strong&gt;Gather.Town&lt;/strong&gt; on Day 2 for live Q&amp;#x26;As. Please find your poster, stand in front of it, and wait for others to come over for discussions in the Poster Rooms. You are suggested to use “Author of $modelname: $yourname” as the ID for being recognised by other attendees.&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Women in Multimedia Roundtable]]></title><description><![CDATA[The SIGMM EC has decided on a “25 in 25” strategy to strategically increase the participation of women in SIGMM and all its activities. This…]]></description><link>https://www.acmmmasia.org/2021/women-in-multimedia/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/women-in-multimedia/</guid><pubDate>Thu, 25 Nov 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;The SIGMM EC has decided on a &lt;strong&gt;&lt;a href=&quot;https://records.sigmm.org/2019/10/21/introducing-the-new-role-of-the-director-of-diversity-and-outreach/&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;“25 in 25”&lt;/em&gt;&lt;/a&gt;&lt;/strong&gt; strategy to strategically increase the participation of women in SIGMM and all its activities. This strategy aims at increasing the participation of women in all activities and committees of SIGMM to at least 25% by 2025.&lt;/p&gt;
&lt;p&gt;ACM Multimedia Asia 2021 is advocating for a better engagement and involvement of female researchers in all sorts of activities in Multimedia Asia and beyond. The Roundtable (scheduled on Day 2) invites six female panellists at different career stages to share their experiences at being a researcher and educator in multimedia. It will also be an event where participants, both women and men can meet up and exchange ideas.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&quot;keynote-text&quot;&gt;
    &lt;div class=&quot;keynote-img&quot;&gt;
        &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 160px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/fb59b8d0cb498b97d7235c41c73436de/e4ec8/Si.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGQABAQEAAwAAAAAAAAAAAAAAAAQDAQUG/8QAFgEBAQEAAAAAAAAAAAAAAAAAAwAC/9oADAMBAAIQAxAAAAGbXKwHjXNXn+RB7QGn/8QAGxAAAwADAQEAAAAAAAAAAAAAAQIDABESEyL/2gAIAQEAAQUCoy5xoFtFaq59UwRlQKdMT9Rc8f/EABgRAQEAAwAAAAAAAAAAAAAAAAEAEBJB/9oACAEDAQE/AR7buC//xAAZEQADAAMAAAAAAAAAAAAAAAAAAREQMUH/2gAIAQIBAT8BnCLD2f/EAB0QAAICAgMBAAAAAAAAAAAAAAERAAIQcTEyYYH/2gAIAQEABj8CVSXqAveBU1Wp35jLcB9n3H//xAAaEAEAAwEBAQAAAAAAAAAAAAABABEhQTFh/9oACAEBAAE/ISCvcNjOJvwzKmZlt9Mwqj4rpAb8FOxgvQTY/UDl7P/aAAwDAQACAAMAAAAQyA8A/8QAGBEAAwEBAAAAAAAAAAAAAAAAAAERQTH/2gAIAQMBAT8QqqY8DDlH/8QAGBEAAwEBAAAAAAAAAAAAAAAAAAERMUH/2gAIAQIBAT8QbJqCGnTY/8QAHRABAAIBBQEAAAAAAAAAAAAAAQARITFBUYGRcf/aAAgBAQABPxCkeoAUOvceA+HO53Duj5KLENrJZLxxAGUJBMnzaoyyDSRDrUVnKHsZLhUxtmV6ksz/2Q==&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Si Liu&quot; title=&quot;Si Liu&quot; src=&quot;/2021/static/fb59b8d0cb498b97d7235c41c73436de/e4ec8/Si.jpg&quot; srcset=&quot;/2021/static/fb59b8d0cb498b97d7235c41c73436de/e4ec8/Si.jpg 160w&quot; sizes=&quot;(max-width: 160px) 100vw, 160px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;/div&gt;
    &lt;!-- - **Title**: --&gt;
    &lt;h3 class=&quot;name&quot;&gt;Si Liu (Panel Chair)&lt;/h3&gt;
    &lt;!-- - **Date**: --&gt;
    &lt;!-- - **Abstract**:  --&gt;
    &lt;p class=&quot;subtitle&quot;&gt;Beihang University, China&lt;/p&gt;
    &lt;p&gt;
        Dr. Si Liu is the leader of the CoLab and an associate professor in Beihang University. She was a visiting professor in Microsoft Research Asia. She used to work with Prof. Shuicheng Yan in National University of Singapore. She obtained Ph.D. degree from Institute of Automation, Chinese Academy of Sciences (CASIA), under the supervision of Prof. Hanqing Lu. She obtained Bachelor degree from Advanced Class of Beijing Institute of Technology (BIT).
    &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&quot;keynote-text&quot;&gt;
    &lt;div class=&quot;keynote-img&quot;&gt;
        &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 160px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/4be7d91995ad834b77e3082fab9967f0/e4ec8/Lina.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGQABAQADAQAAAAAAAAAAAAAAAAQBAgUG/8QAFgEBAQEAAAAAAAAAAAAAAAAAAgEA/9oADAMBAAIQAxAAAAG6anUqxgj5W4u54U//xAAZEAADAQEBAAAAAAAAAAAAAAABAgMRABD/2gAIAQEAAQUCuWEgGn5VlVQ4Vd3pkmtiTLe//8QAFhEAAwAAAAAAAAAAAAAAAAAAASAx/9oACAEDAQE/ARE//8QAFREBAQAAAAAAAAAAAAAAAAAAEEH/2gAIAQIBAT8BpT//xAAdEAABBAIDAAAAAAAAAAAAAAABAAIRIRAxAyJB/9oACAEBAAY/AutE0g4chcPQcQdnSl+lSE2m4//EABwQAQACAwADAAAAAAAAAAAAAAEAESExURBBYf/aAAgBAQABPyF1e0W5Mj2L3rvgwuTCEMCdpAC0J0iIrLtjo/II0z//2gAMAwEAAgADAAAAECsIf//EABcRAQEBAQAAAAAAAAAAAAAAAAEQETH/2gAIAQMBAT8QBpYcn//EABcRAQEBAQAAAAAAAAAAAAAAAAEQETH/2gAIAQIBAT8QVwEeJ//EAB4QAQADAAICAwAAAAAAAAAAAAEAESFRYTFxgZGx/9oACAEBAAE/EM0xOJdfoY3UAutNI9XEpqXgCU6rz67j3ZLQBdPiH2rwljFH6iWscHa/LAgQOmp//9k=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Lina Yao&quot; title=&quot;Lina Yao&quot; src=&quot;/2021/static/4be7d91995ad834b77e3082fab9967f0/e4ec8/Lina.jpg&quot; srcset=&quot;/2021/static/4be7d91995ad834b77e3082fab9967f0/e4ec8/Lina.jpg 160w&quot; sizes=&quot;(max-width: 160px) 100vw, 160px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;/div&gt;
    &lt;!-- - **Title**: --&gt;
    &lt;h3 class=&quot;name&quot;&gt;Lina Yao&lt;/h3&gt;
    &lt;!-- - **Date**: --&gt;
    &lt;!-- - **Abstract**:  --&gt;
    &lt;p class=&quot;subtitle&quot;&gt;University of New South Wales, Australia&lt;/p&gt;
    &lt;p&gt;
        Lina Yao is an Associate Professor at School of Computer Science and Engineering at the University of New South Wales (UNSW), Australia. Her research interest lies in machine learning, and its applications in recommender systems, activity modeling and prediction, the Internet of Things, and Brain-Computer Interface. She is serving as the Associate Editor for ACM Transactions on Sensor Networks (ACM TOSN) and Knowledge-based Systems (KNOSYS).
    &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&quot;keynote-text&quot;&gt;
    &lt;div class=&quot;keynote-img&quot;&gt;
        &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 160px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/71443830fe8fad2a0f7cd7461c6a06cf/e4ec8/lianli.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMEAf/EABYBAQEBAAAAAAAAAAAAAAAAAAEAAv/aAAwDAQACEAMQAAABrnpwdQSGQ2BX/8QAHBAAAgIDAQEAAAAAAAAAAAAAAQIDBAAREiEz/9oACAEBAAEFApSQpQwgZO/GWW0F9FbTG194ief/xAAVEQEBAAAAAAAAAAAAAAAAAAABIP/aAAgBAwEBPwEj/8QAFhEBAQEAAAAAAAAAAAAAAAAAARAx/9oACAECAQE/AWGz/8QAGxAAAgMBAQEAAAAAAAAAAAAAAQIAETEhEBL/2gAIAQEABj8CpdPIG+ie98WtuJeX4zMoJuNNn//EABoQAQADAQEBAAAAAAAAAAAAAAEAESExEEH/2gAIAQEAAT8hCdek2VxV5vgbNEoRhdLJEWHGZUDSnI8/hhL/AF2f/9oADAMBAAIAAwAAABAEAAD/xAAXEQEBAQEAAAAAAAAAAAAAAAABEBEx/9oACAEDAQE/EAYseT//xAAYEQEAAwEAAAAAAAAAAAAAAAABABARMf/aAAgBAgEBPxBOhY9n/8QAHhABAAMAAQUBAAAAAAAAAAAAAQARITFBUWFxkdH/2gAIAQEAAT8QCCy+HqvwY+UEuNOy+0GPuGUDgdK/TI+CFeioQS16D4hJHa0cPHyItVUB6FGQQBhkHjCf/9k=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Lianli Gao&quot; title=&quot;Lianli Gao&quot; src=&quot;/2021/static/71443830fe8fad2a0f7cd7461c6a06cf/e4ec8/lianli.jpg&quot; srcset=&quot;/2021/static/71443830fe8fad2a0f7cd7461c6a06cf/e4ec8/lianli.jpg 160w&quot; sizes=&quot;(max-width: 160px) 100vw, 160px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;/div&gt;
    &lt;!-- - **Title**: --&gt;
    &lt;h3 class=&quot;name&quot;&gt;Lianli Gao&lt;/h3&gt;
    &lt;!-- - **Date**: --&gt;
    &lt;!-- - **Abstract**:  --&gt;
    &lt;p class=&quot;subtitle&quot;&gt;University of Electronic Science and Technology of China, China&lt;/p&gt;
    &lt;p&gt;
        Dr. Lianli Gao is a Professor at School of Computer Science and Engineering, UESTC. She is a member of the CFM Lab. She obtained her PhD degree in Information Technology from The University of Queensland (UQ), Australia, under the supervision of  Prof. Jane Hunter and Prof. Michael Bruenig. She received her BS degree in Computer Science from UESTC, in 2009. 
        Her research ranges from Semantic Web, Machine Learning, Deep Learning, Computer Vision (Images and Videos), NLP, Knowledge Reasoning, Knowledge and the related practical applications etc. Specifically, she is mainly focusing on integrating Natural Language for Visual Content Understanding. 
    &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&quot;keynote-text&quot;&gt;
    &lt;div class=&quot;keynote-img&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 160px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/cf4b127742f9fdf60adf4ec27f13df56/e4ec8/Mahsa.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGQABAAMBAQAAAAAAAAAAAAAAAAIEBQED/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAasb0DOdWWvcjMFf/8QAHBAAAgICAwAAAAAAAAAAAAAAAQMAAgQhEBEx/9oACAEBAAEFAqqNgxZrwnaLjuH3GJDXaE//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAcEAAABwEBAAAAAAAAAAAAAAAAAQIQESExQWH/2gAIAQEABj8CL0a23IVbRwwpv//EAB0QAAMAAQUBAAAAAAAAAAAAAAABESEQMUFRgWH/2gAIAQEAAT8hcS9Edqux4ZMlCQVV4JqbCwv0RkFzp//aAAwDAQACAAMAAAAQSAD8/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPxAf/8QAFhEBAQEAAAAAAAAAAAAAAAAAEAEx/9oACAECAQE/EC6f/8QAHBABAAIDAAMAAAAAAAAAAAAAAQARITFREEGh/9oACAEBAAE/ENiqdsBBQT0QYGHJIunJeSOdWteldnwqKZhTtQpFrKKfH//Z&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Mahsa Baktashmotlagh&quot; title=&quot;Mahsa Baktashmotlagh&quot; src=&quot;/2021/static/cf4b127742f9fdf60adf4ec27f13df56/e4ec8/Mahsa.jpg&quot; srcset=&quot;/2021/static/cf4b127742f9fdf60adf4ec27f13df56/e4ec8/Mahsa.jpg 160w&quot; sizes=&quot;(max-width: 160px) 100vw, 160px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;/div&gt;
    &lt;!-- - **Title**: --&gt;
    &lt;h3 class=&quot;name&quot;&gt;Mahsa Baktashmotlagh&lt;/h3&gt;
    &lt;!-- - **Date**: --&gt;
    &lt;!-- - **Abstract**:  --&gt;
    &lt;p class=&quot;subtitle&quot;&gt;The University of Queensland, Australia&lt;/p&gt;
    &lt;p&gt;
        Mahsa Baktashmotlagh is currently a Lecturer at UQ with a research focus, developing machine learning and datamining techniques applied in: Visual data analysis (Visual domain adaptation, video classification, and animal’s foragingbehavioural analysis), Road traffic networks (Mining large scale road traffic networks and building a road loadbalancing tool to predict congestion on any road in the city) , Biomedical data (Prediction of neonatal sepsis), and Finance (Hedging foreign exchange trading risks).
    &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&quot;keynote-text&quot;&gt;
    &lt;div class=&quot;keynote-img&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 160px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/43a6d603847f21399076a6fb4ab3fffd/e4ec8/Ran.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGQABAAMBAQAAAAAAAAAAAAAAAAMEBQIG/8QAFQEBAQAAAAAAAAAAAAAAAAAAAQL/2gAMAwEAAhADEAAAAbWNbkK2EBjykwVwKP/EABsQAAMAAwEBAAAAAAAAAAAAAAECAwQRIgAS/9oACAEBAAEFAs08qvMW+5Vpu9SqzwzvGc+sSQKOo//EABURAQEAAAAAAAAAAAAAAAAAAAEg/9oACAEDAQE/ASP/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAaEAEBAQEAAwAAAAAAAAAAAAABAAIRMVGR/9oACAEBAAY/AgtNnXsgfEvyxdsXDSX/xAAbEAEAAwEBAQEAAAAAAAAAAAABABEhMXFBUf/aAAgBAQABPyHqM6zQXfyKt3SFLqYFhXqeSyMonYoL0uVh34M//9oADAMBAAIAAwAAABDg4H3/xAAYEQEBAAMAAAAAAAAAAAAAAAAAASExUf/aAAgBAwEBPxCMLt1X/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQAQMf/aAAgBAgEBPxBcekX/xAAcEAEAAgMAAwAAAAAAAAAAAAABABEhMUFRcYH/2gAIAQEAAT8QcKwNB15KqKC8HcQywq8vZX+tVExeiPb3wsTZMlfxYvjAbiKnKt7iH7RW2E//2Q==&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Ran Yi&quot; title=&quot;Ran Yi&quot; src=&quot;/2021/static/43a6d603847f21399076a6fb4ab3fffd/e4ec8/Ran.jpg&quot; srcset=&quot;/2021/static/43a6d603847f21399076a6fb4ab3fffd/e4ec8/Ran.jpg 160w&quot; sizes=&quot;(max-width: 160px) 100vw, 160px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;/div&gt;
    &lt;!-- - **Title**: --&gt;
    &lt;h3 class=&quot;name&quot;&gt;Ran Yi&lt;/h3&gt;
    &lt;!-- - **Date**: --&gt;
    &lt;!-- - **Abstract**:  --&gt;
    &lt;p class=&quot;subtitle&quot;&gt;Shanghai Jiao Tong University, China&lt;/p&gt;
    &lt;p&gt;
        Ran Yi is an Assistant Professor at Shanghai Jiao Tong University (SJTU). She is a member of SJTU Digital Media and Computer Vision Laboratory. She received her Ph.D. degree from CSCG Group, Tsinghua University in 2021, advised by Prof. Yong-Jin Liu. She also closely collaborates with Prof. Yu-Kun Lai, Prof. Paul L. Rosin and Prof. Ying He.
    &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&quot;keynote-text&quot;&gt;
    &lt;div class=&quot;keynote-img&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 160px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/1959292d17b863ba8cfa2863d414cb19/e4ec8/laikuan.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAQFA//EABUBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAHVzqIprfTqnJ4g0BP/xAAeEAACAgICAwAAAAAAAAAAAAABAgADBBEFEhMjMf/aAAgBAQABBQLIsKIbGpn2Zh6y9/Xj78HIE99BpWxVf//EABcRAQADAAAAAAAAAAAAAAAAABABMUH/2gAIAQMBAT8BijD/xAAVEQEBAAAAAAAAAAAAAAAAAAABIP/aAAgBAgEBPwFj/8QAHhAAAQQCAwEAAAAAAAAAAAAAAQACEUEhUQMQEjH/2gAIAQEABj8Cx9OEHey4WD1xuoFRtMnSAoLNBQDAX//EABoQAQEBAQEBAQAAAAAAAAAAAAERADEhQVH/2gAIAQEAAT8hBDjw/mYP6kvmEAnHWP0XDdD87tq4q3woZkjwIasQPDf/2gAMAwEAAgADAAAAEM8Ivv/EABcRAQEBAQAAAAAAAAAAAAAAAAEQETH/2gAIAQMBAT8QArYdT//EABcRAQEBAQAAAAAAAAAAAAAAAAEQEUH/2gAIAQIBAT8QTs6T/8QAHxABAAIDAAEFAAAAAAAAAAAAAQARITFBUWFxgaHx/9oACAEBAAE/EAFDq9Fr8BG7tMEV08MUqwsfJM+OMvZ+y720KGuwgiEijft9RYOjwW7Y6Ll7MZM3NE8EoJ//2Q==&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Lai Kuan Wong&quot; title=&quot;Lai Kuan Wong&quot; src=&quot;/2021/static/1959292d17b863ba8cfa2863d414cb19/e4ec8/laikuan.jpg&quot; srcset=&quot;/2021/static/1959292d17b863ba8cfa2863d414cb19/e4ec8/laikuan.jpg 160w&quot; sizes=&quot;(max-width: 160px) 100vw, 160px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;/div&gt;
    &lt;!-- - **Title**: --&gt;
    &lt;h3 class=&quot;name&quot;&gt;Lai-Kuan Wong&lt;/h3&gt;
    &lt;!-- - **Date**: --&gt;
    &lt;!-- - **Abstract**:  --&gt;
    &lt;p class=&quot;subtitle&quot;&gt;Multimedia University, Malaysia&lt;/p&gt;
    &lt;p&gt;
        Lai-Kuan Wong is currently an Associate Professor and Chair of the Centre for Visual Computing (CVC) at Multimedia University, Malaysia, where she also served as the Deputy Dean of Research and Innovation for Faculty of Computing and Informatics in 2018-2021. She received her Ph.D. in Computer Science from National University of Singapore. Her research interests include computational photography and computational aesthetics, 3D scene analysis and understanding, and medical imaging. Lai-Kuan has co-chaired several workshops (Workshops at ACM Multimedia 2020, ACCV 2018, ACPR 2015), and regularly serves as the technical program committee for reputable conferences (CVPR, AAAI, WACV, ACM Multimedia, MMM) and as a reviewer for several IEEE Transactions. She also serves as the organizing committee for several conferences including the upcoming IEEE ICME 2022, IEEE ISPACS 2022, and IEEE ICIP 2023. She is the Technical Committee for APSIPA SPS TC since 2018.
    &lt;/p&gt;
&lt;/div&gt;
&lt;style&gt;
    .keynote-text {
        text-align: left;
    }
    .keynote-img {
        float: none;
        margin-right: 1.8rem;
        margin-bottom: 1rem;
        width: 160px;
    }
    .note {
        font-size: 14px;
        color: grey;
    }
    .name {
        margin-bottom: 0.5rem !important;
    }

    @media (min-width: 768px) {
        .keynote-text {
            text-align: justify;
        }
        .keynote-img {
            float: left;
        }
    }
&lt;/style&gt;</content:encoded></item><item><title><![CDATA[Registration Information]]></title><description><![CDATA[Registration Procedures Register your information in the system here and make payment. A confirmation email with a Payment Receipt ID will…]]></description><link>https://www.acmmmasia.org/2021/registration/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/registration/</guid><pubDate>Mon, 15 Nov 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Registration Procedures&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Register your information in the system &lt;strong&gt;&lt;a href=&quot;https://payments.uq.edu.au/onestopweb/ECET101EAI017&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;here&lt;/em&gt;&lt;/a&gt;&lt;/strong&gt; and make payment.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;A confirmation email with a Payment Receipt ID will be sent to you by email after you finish a registration.&lt;/li&gt;
&lt;li&gt;With the Payment Receipt ID, you can check your registration information. For modification, please send your request to &lt;strong&gt;&lt;a href=&quot;mailto:mmasia21@itee.uq.edu.au&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;mmasia21@itee.uq.edu.au&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Registration Fees&lt;/h2&gt;
&lt;div class=&quot;mb-6 w-full overflow-x-scroll md:overflow-hidden&quot;&gt;
    &lt;table class=&quot;table-custom mb-0 text-center table-auto overflow-x-scroll md:overflow-hidden&quot;&gt;
        &lt;tbody class=&quot;border-t text-gray-600 text-sm font-normal&quot;&gt;
            &lt;tr class=&quot;bg-gray-100&quot;&gt;
                &lt;td rowspan=&quot;6&quot; class=&quot;font-bold&quot;&gt;AUTHOR&lt;br/&gt;Registration&lt;br/&gt;&lt;br/&gt;Main +&lt;br/&gt;Workshop +&lt;br/&gt;PhD School&lt;/td&gt;
                &lt;td rowspan=&quot;3&quot;&gt;&lt;span class=&quot;font-bold&quot;&gt;EARLY BIRD&lt;/span&gt;&lt;br/&gt;Until 21 November 2021&lt;/td&gt;
                &lt;td class=&quot;font-bold&quot;&gt;Category – Author&lt;/td&gt;
                &lt;td colspan=&quot;2&quot; class=&quot;font-bold&quot;&gt;Online&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;Author &lt;span class=&quot;font-semibold&quot;&gt;ACM/SIGMM Member&lt;/span&gt;&lt;br/&gt;[Either Professional or Student]&lt;/td&gt;
                &lt;td colspan=&quot;2&quot;&gt;AU$380 &lt;br/&gt; (US$250)&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;Author Non-Member&lt;/td&gt;
                &lt;td colspan=&quot;2&quot;&gt;AU$455&lt;br/&gt;(US$300)&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr class=&quot;bg-gray-100&quot;&gt;
                &lt;td rowspan=&quot;3&quot;&gt;&lt;span class=&quot;font-bold&quot;&gt;LATE&lt;/span&gt;&lt;br/&gt;After 21 November 2021&lt;/td&gt;
                &lt;td class=&quot;font-bold&quot;&gt;Category – Author&lt;/td&gt;
                &lt;td colspan=&quot;2&quot; class=&quot;font-bold&quot;&gt;Online&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;Author &lt;span class=&quot;font-semibold&quot;&gt;ACM/SIGMM Member&lt;/span&gt;&lt;br/&gt;[Either Professional or Student]&lt;/td&gt;
                &lt;td colspan=&quot;2&quot;&gt;AU$530&lt;br/&gt;(US$350)&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;Author Non-Member&lt;/td&gt;
                &lt;td colspan=&quot;2&quot;&gt;AU$605&lt;br/&gt;(US$400)&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr class=&quot;bg-gray-100&quot;&gt;
                &lt;td rowspan=&quot;2&quot; colspan=&quot;2&quot; class=&quot;font-bold&quot;&gt;ATTENDEE&lt;br/&gt;Main + Workshop + PhD School&lt;/td&gt;
                &lt;td class=&quot;font-bold&quot;&gt;Category - Attendee&lt;/td&gt;
                &lt;td class=&quot;font-bold&quot;&gt;Online&lt;/td&gt;
                &lt;td class=&quot;font-bold&quot;&gt;In Person&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td&gt;Conference Attendee&lt;/td&gt;
                &lt;td&gt;AU$75&lt;br/&gt;(US$50)&lt;/td&gt;
                &lt;td&gt;AU$305&lt;br/&gt;(US$200)&lt;/td&gt;
            &lt;/tr&gt;
        &lt;/tbody&gt;
    &lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;*All pricing is &lt;ins&gt;inclusive of GST&lt;/ins&gt; and is given in &lt;strong&gt;Australian dollars&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Registration Policy&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;AUD rate is used for all registrants.&lt;/li&gt;
&lt;li&gt;Full registration includes ACM Member or Non-Member registration rates.&lt;/li&gt;
&lt;li&gt;Each accepted paper must be covered by at least one full registration &lt;strong&gt;by 21 November&lt;/strong&gt;, and one full registration can &lt;strong&gt;cover ONE paper only&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Each conference registration includes all conference sessions including workshops and PhD School, and one copy of the electronic proceedings.&lt;/li&gt;
&lt;li&gt;If you register for the ACM discounted rate, you are required to provide valid ACM member number. Please make sure that your membership has been renewed for 2021.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;ACM/SIGMM Professional or Student Membership&lt;/h2&gt;
&lt;p&gt;A professional or student membership is required for Registration at the discounted Member Rate. To join SigMM, please &lt;strong&gt;&lt;a href=&quot;https://services.acm.org/public/qj/gensigqj/login_gensigqj.cfm?rdr=promo=QJSIG&amp;#x26;offering=044&amp;#x26;form_type=SIG&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;visit this link&lt;/em&gt;&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Registration Methods&lt;/h2&gt;
&lt;h3&gt;Online&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ACM Multimedia Asia 2021 prefers that participants register through the online registration system, which accepts Master card, Visa card and American Express.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Online registration link:&lt;/strong&gt; &lt;strong&gt;&lt;a href=&quot;https://payments.uq.edu.au/onestopweb/ECET101EAI017&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://payments.uq.edu.au/onestopweb/ECET101EAI017&lt;/em&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Other&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If online payment via credit card is not possible, contact &lt;strong&gt;&lt;a href=&quot;mailto:mmasia21@itee.uq.edu.au&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;mmasia21@itee.uq.edu.au&lt;/a&gt;&lt;/strong&gt; to make alternative arrangements.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Registration Cancellation Policy&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cancellation and refund requests received on or before 21 November 2021 will be refunded at 50% of the registration fee.&lt;/li&gt;
&lt;li&gt;Cancellation and refund requests received from 22 November 2021 will not be refunded.&lt;/li&gt;
&lt;li&gt;Alternatively, if you cannot attend, your registration may be transferred to a colleague. To organize a registration name transfer, email your request to &lt;strong&gt;&lt;a href=&quot;mailto:mmasia21@itee.uq.edu.au&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;mmasia21@itee.uq.edu.au&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Contact Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;For any registration-related issues or queries please send an email to: &lt;strong&gt;&lt;a href=&quot;mailto:mmasia21@itee.uq.edu.au&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;mmasia21@itee.uq.edu.au&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Conference Keynotes]]></title><description><![CDATA[Keynote 1: Privacy-aware Multimedia Analytics Abstract: In this talk, we present our research on privacy-aware multimedia analytics. We will…]]></description><link>https://www.acmmmasia.org/2021/conference-keynotes/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/conference-keynotes/</guid><pubDate>Sun, 14 Nov 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Keynote 1: Privacy-aware Multimedia Analytics&lt;/h2&gt;
&lt;!-- ### Presenter --&gt;
&lt;div class=&quot;keynote-text&quot;&gt;
    &lt;div class=&quot;keynote-img&quot;&gt;
        &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 251px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/fc23db65dfb4ea91cf6292f2f078bb7d/c6c63/mohan_.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAQF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAgH/2gAMAwEAAhADEAAAAdeWyErUCOaCrBT/AP/EABsQAQACAgMAAAAAAAAAAAAAAAECAwAEEBEz/9oACAEBAAEFAr5sK6bVctCVesPC9uv5mf/EABcRAQADAAAAAAAAAAAAAAAAABABMUH/2gAIAQMBAT8BijD/xAAWEQADAAAAAAAAAAAAAAAAAAABIDH/2gAIAQIBAT8BNT//xAAZEAEAAwEBAAAAAAAAAAAAAAABABAhEUH/2gAIAQEABj8C6Q1e0jN8rb//xAAbEAEAAgMBAQAAAAAAAAAAAAABABEQITFRkf/aAAgBAQABPyFGe+Qmo9WYcCr4zVq17wzKifpi1P/aAAwDAQACAAMAAAAQs8gC/8QAFxEAAwEAAAAAAAAAAAAAAAAAARARMf/aAAgBAwEBPxCKqGl//8QAFhEBAQEAAAAAAAAAAAAAAAAAEBFB/9oACAECAQE/EKg0/8QAHBABAQADAQADAAAAAAAAAAAAAREAIUFhEHGx/9oACAEBAAE/EL6LAyy9yt4G1ATSP38Cvls48xAgoinvJgiUcfQo68y412093iEXr+5//9k=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Mohan Kankanhalli&quot; title=&quot;Mohan Kankanhalli&quot; src=&quot;/2021/static/fc23db65dfb4ea91cf6292f2f078bb7d/c6c63/mohan_.jpg&quot; srcset=&quot;/2021/static/fc23db65dfb4ea91cf6292f2f078bb7d/a6b4f/mohan_.jpg 200w,
/2021/static/fc23db65dfb4ea91cf6292f2f078bb7d/c6c63/mohan_.jpg 251w&quot; sizes=&quot;(max-width: 251px) 100vw, 251px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;/div&gt;
    &lt;!-- - **Title**: --&gt;
    &lt;h3&gt;Mohan Kankanhalli&lt;/h3&gt;
    &lt;!-- - **Date**: --&gt;
    &lt;!-- - **Abstract**:  --&gt;
    &lt;p&gt;School of Computing, National University of Singapore&lt;/p&gt;
    &lt;p&gt;
        Mohan Kankanhalli is Provost&apos;s Chair Professor of Computer Science at the National University of Singapore (NUS). He is also the Dean of NUS School of Computing. Before becoming the Dean in July 2016, he was the NUS Vice Provost (Graduate Education) during 2014-2016 and Associate Provost during 2011-2013. Mohan obtained his BTech from IIT Kharagpur and MS &amp;amp; PhD from the Rensselaer Polytechnic Institute. Mohan’s research interests are in Multimedia Computing, Computer Vision, Information Security &amp;amp; Privacy and Image/Video Processing. He has made many contributions in the area of multimedia &amp;amp; vision – image and video understanding, data fusion, visual saliency as well as in multimedia security – content authentication and privacy, multi-camera surveillance.
    &lt;/p&gt;
    &lt;p&gt;
        He directs N-CRiPT (NUS Centre for Research in Privacy Technologies) which conducts research on privacy on structured as well as unstructured (multimedia, sensors, IoT) data. N-CRiPT looks at privacy at both individual and organizational levels along the entire data life cycle. He is personally involved in privacy research related to images, video and social media as well as privacy risk management. N-CRiPT, which has been funded by Singapore’s National Research Foundation, works with many industry, government and academic partners. Mohan is a Fellow of IEEE.
    &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: In this talk, we present our research on privacy-aware multimedia analytics. We will present three works covering different aspects of multimedia analytics.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first work is about privacy protection against machines. Utilizing machine learning and big data, algorithms often act as a tool for privacy violation, by automatically selecting content with sensitive information, such as photos that contain faces or vehicle license plates. The key idea is to perturb images using adversarial machine learning to protect image attributes privacy, while ensuring the images are not degraded. We conducted an experimental study to explore factors that influence human sensitivity to visual changes, which led to the concept of a human sensitivity map. Using this map, a human-sensitivity-aware image perturbation model is developed that can subtly alter an image such that sensitive attributes like gender are misclassified.&lt;/li&gt;
&lt;li&gt;The second work concerns privacy-preserving analytics on images. Attributes such as emotions, gender and age in images and videos are important for many applications. Existing methods extract this information from faces in the images. However, faces raise serious privacy concerns as they reveal people’s identity. We first did an eye-tracking based human study of age, gender, and emotion prediction of people in images under various identity preserving scenarios - obfuscating eyes, lower face, head or the full face. Motivated by this study, we successfully developed a deep learning model for attributes prediction under privacy-preserving conditions and we present its results.&lt;/li&gt;
&lt;li&gt;The third work concerns training machine learning models where data sets cannot be shared due to privacy regulations (e.g., from medical studies). A simple yet unconventional approach for anonymized data synthesis can enable third parties to benefit from such valuable data. We propose learning implicitly from visually unrealistic, task-relevant stimuli, which are synthesized by exciting the neurons of a trained neural network. Neuronal excitation serves as a pseudo-generative model, and can be extended to inhibit representations that are associated with specific individuals, thus providing privacy. The stimuli data is then used to train new classification models. Experiments on MNIST and sleep apnea data show that these models offer protection against adversarial association and membership inference attacks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will end with a general discussion on privacy concerns related to multimedia analytics.&lt;/p&gt;
&lt;h2&gt;Keynote 2: Artificial Intelligence: Paving a Path to Digital Economy Transformation&lt;/h2&gt;
&lt;!-- ### Presenter --&gt;
&lt;div class=&quot;keynote-text&quot;&gt;
    &lt;div class=&quot;keynote-img&quot;&gt;
        &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 251px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/ea4e626ed263e964ff3ae40a0fc9a2ce/c6c63/yong_.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAQDBf/EABUBAQEAAAAAAAAAAAAAAAAAAAEA/9oADAMBAAIQAxAAAAHkVZ0jzWhMbxpQX//EABoQAAIDAQEAAAAAAAAAAAAAAAABAgMhEBP/2gAIAQEAAQUCPJOowjlXHJuo/8QAFREBAQAAAAAAAAAAAAAAAAAAASD/2gAIAQMBAT8BI//EABURAQEAAAAAAAAAAAAAAAAAAAEg/9oACAECAQE/AWP/xAAaEAEAAgMBAAAAAAAAAAAAAAABECEAElGB/9oACAEBAAY/As2GyV8lvkf/xAAbEAACAgMBAAAAAAAAAAAAAAABEQAhMVGBEP/aAAgBAQABPyFR5RGtxQoLhBUONwi5k3GSZBx55//aAAwDAQACAAMAAAAQBwc+/8QAFhEBAQEAAAAAAAAAAAAAAAAAAREg/9oACAEDAQE/EARuP//EABYRAQEBAAAAAAAAAAAAAAAAABABEf/aAAgBAgEBPxDRD//EABwQAQADAAMBAQAAAAAAAAAAAAEAESExQVEQwf/aAAgBAQABPxBYGctQCnZSNO46lSIFSj1inuL1w6bOtLbkr7DhgSuNT+fP/9k=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Yong Rui&quot; title=&quot;Yong Rui&quot; src=&quot;/2021/static/ea4e626ed263e964ff3ae40a0fc9a2ce/c6c63/yong_.jpg&quot; srcset=&quot;/2021/static/ea4e626ed263e964ff3ae40a0fc9a2ce/a6b4f/yong_.jpg 200w,
/2021/static/ea4e626ed263e964ff3ae40a0fc9a2ce/c6c63/yong_.jpg 251w&quot; sizes=&quot;(max-width: 251px) 100vw, 251px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;/div&gt;
    &lt;!-- - **Title**: --&gt;
    &lt;h3&gt;Yong Rui&lt;/h3&gt;
    &lt;!-- - **Date**: --&gt;
    &lt;!-- - **Abstract**:  --&gt;
    &lt;p&gt;Lenovo Group&lt;/p&gt;
    &lt;p&gt;
        Dr. Yong Rui is the Corporate CTO and Senior Vice President of Lenovo Group. He directs Lenovo’s technical strategies and R&amp;amp;D directions. Additionally, Dr. Rui leads the Lenovo Research organization that investigates intelligent devices, artificial intelligence, 5G, cloud and edge computing, and smart vertical solutions. Prior to joining Lenovo, Dr. Rui spent 18 years with Microsoft where he held various leadership roles in R&amp;amp;D strategy, basic research, technology incubation and product development, and most recently served as Deputy Managing Director of Microsoft Research Asia.
    &lt;/p&gt;
    &lt;p&gt;
        A Fellow of ACM, IEEE, IAPR and SPIE, and a Foreign Member of Academia Europaea and Canadian Academy of Engineering, Dr. Rui is recognized as a leading expert in AI and multimedia analysis. He is a recipient of many awards, including the 2018 ACM SIGMM Technical Achievement Award, the 2017 IEEE SMC Society Andrew P. Sage Best Transactions Paper Award, the 2017 ACM TOMM Nicolas Georganas Best Paper Award, the 2016 IEEE Computer Society Edward J. McCluskey Technical Achievement Award, the 2016 IEEE Signal Processing Society Best Paper Award and the 2010 Most Cited Paper of the Decade Award from Journal of Visual Communication and Image Representation. He holds 70 issued patents, has published 4 books, 12 book chapters, and 200 refereed journal and conference papers. With over 30,000+ citations, and an h-Index of 82, his publications are among the most referenced.
    &lt;/p&gt;
    &lt;p&gt;
        Dr. Rui is an Associate Editor of ACM Trans. on Multimedia Computing, Communication and Applications (TOMM)  (2007- ), and a founding Editor of International Journal of Multimedia Information Retrieval (2011- ). He was the Editor-in-Chief of IEEE MultiMedia magazine (2014-2017), and an Associate Editor of IEEE Access (2013-2016), IEEE Trans. on Multimedia (2004-2008), IEEE Trans. on Circuits and Systems for Video Technologies (2006-2010), ACM/Springer Multimedia Systems Journal (2004-2006), and International Journal of Multimedia Tools and Applications (2004-2006). He also served on the Advisory Board of IEEE Trans. on Automation Science and Engineering (2006-2016).
    &lt;/p&gt;
    &lt;p&gt;
        Involved in many facets of the field, Dr. Rui is a member of numerous organizing and program committees for conferences including ACM Multimedia, ACM ICMR, IEEE ICME, SPIE ITCom, and ICPR. He is General Co-Chair of ACM Multimedia in 2009 and 2014, ACM ICMR in 2006 and 2012, and ICIMCS in 2010, and Program Co-Chair of ACM Multimedia in 2006, Pacific Rim Multimedia (PCM) in 2006, and IEEE ICME in 2009. He is on the Steering Committees of ACM Multimedia, ACM ICMR, IEEE ICME and PCM. He is an Executive Member of ACM SIGMM (2009-2010, 2013-2016), and the founding Chair of its China Chapter.
    &lt;/p&gt;
    &lt;p&gt;
        Dr. Rui received his BS from Southeast University Summa cum laude, his MS from Tsinghua University, and his PhD from University of Illinois at Urbana-Champaign (UIUC). 
    &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: With the rapid growth of digital economy, the world is entering a new era of digital transformation. Technologies like the Artificial Intelligence are changing the way we live and work profoundly as we know it. In his talk, we will demonstrate how Artificial Intelligence is empowering the entire value chain of industrial digitalization, using Lenovo’s own intelligent transformation practices in smart manufacturing as an example.  We will also look into how the three elements of AI (data, algorithm and computing power) will change in future years.&lt;/p&gt;
&lt;h2&gt;Keynote 3: How to do Research for Fun and Profit&lt;/h2&gt;
&lt;!-- ### Presenter --&gt;
&lt;div class=&quot;keynote-text&quot;&gt;
    &lt;div class=&quot;keynote-img&quot;&gt;
        &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 251px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/7e2c99d00d4809d8364aeb3f21bcf3b8/c6c63/divesh_.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMEAv/EABYBAQEBAAAAAAAAAAAAAAAAAAECA//aAAwDAQACEAMQAAABjbDslm4TOa5ogv8A/8QAGxABAAIDAQEAAAAAAAAAAAAAAQIDABESIzH/2gAIAQEAAQUCrfIikU5yqQVmhrOj5LpazWf/xAAWEQEBAQAAAAAAAAAAAAAAAAABABH/2gAIAQMBAT8BAsiL/8QAFhEAAwAAAAAAAAAAAAAAAAAAABAS/9oACAECAQE/ASX/AP/EAB4QAAICAQUBAAAAAAAAAAAAAAABAiExESJBUWGB/9oACAEBAAY/AlGGeR6iVMwLY19LlFX0JD8MKz//xAAdEAEAAwACAwEAAAAAAAAAAAABABEhMWFBUZHB/9oACAEBAAE/Ifeves3B9inwV2scC3adi5FFQCuoobZKo+A/I3x6QUtZ1P/aAAwDAQACAAMAAAAQdP8AQ//EABgRAQADAQAAAAAAAAAAAAAAAAABESEx/9oACAEDAQE/EKGruEMf/8QAGBEAAwEBAAAAAAAAAAAAAAAAAAERMUH/2gAIAQIBAT8QnCh6iI//xAAdEAADAQACAwEAAAAAAAAAAAABESEAMWFBUZGB/9oACAEBAAE/EBGzQBtX7k4ADhMu/W5FAkkS8yWyo5LhI8uSwPZ/MomAFHTFfeIQIiHdP7hgiGETOPiAbHkgPf/Z&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Divesh Srivastava&quot; title=&quot;Divesh Srivastava&quot; src=&quot;/2021/static/7e2c99d00d4809d8364aeb3f21bcf3b8/c6c63/divesh_.jpg&quot; srcset=&quot;/2021/static/7e2c99d00d4809d8364aeb3f21bcf3b8/a6b4f/divesh_.jpg 200w,
/2021/static/7e2c99d00d4809d8364aeb3f21bcf3b8/c6c63/divesh_.jpg 251w&quot; sizes=&quot;(max-width: 251px) 100vw, 251px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;/div&gt;
    &lt;!-- - **Title**: --&gt;
    &lt;h3&gt;Divesh Srivastava&lt;/h3&gt;
    &lt;!-- - **Date**: --&gt;
    &lt;!-- - **Abstract**:  --&gt;
    &lt;p&gt;AT&amp;amp;T&lt;/p&gt;
    &lt;p&gt;
        Divesh Srivastava is the Head of Database Research at AT&amp;amp;T. He is a Fellow of the Association for Computing Machinery (ACM), the Vice President of the VLDB Endowment, co-chair of the ACM Publications Board, on the Board of Directors of the Computing Research Association (CRA), and an associate editor of the ACM Transactions on Data Science (TDS). He has served as the managing editor of the Proceedings of the VLDB Endowment (PVLDB), as associate editor of the ACM Transactions on Database Systems (TODS), and as associate Editor-in-Chief of the IEEE Transactions on Knowledge and Data Engineering (TKDE). He has presented keynote talks at several international conferences, and his research interests and publications span a variety of topics in data management. He received his Ph.D. from the University of Wisconsin, Madison, USA, and his Bachelor of Technology from the Indian Institute of Technology, Bombay, India.
    &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: Research is characterized as the process of posing new questions, undertaking a creative and systematic inquiry to find answers, and communicating this knowledge to the community.  In this talk, I present a personal perspective to early career researchers and Ph.D. students at various stages in their careers on what they can do to help ensure that their research efforts are successful, and the process is enjoyable.&lt;/p&gt;
&lt;!-- **Date**: TBA --&gt;
&lt;h2&gt;Keynote 4: Navigation Models for Interactive 360-Degree Video Streaming Systems&lt;/h2&gt;
&lt;!-- ### Presenter --&gt;
&lt;div class=&quot;keynote-text&quot;&gt;
    &lt;div class=&quot;keynote-img&quot;&gt;
        &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 251px; border: solid 0.75px #ddd&quot;&gt;
      &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/2021/static/63ba6c302fae1fc7b713108978be7f8c/c6c63/klara_.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAQCBf/EABQBAQAAAAAAAAAAAAAAAAAAAAL/2gAMAwEAAhADEAAAAYqd3F8JSQmpCshn/8QAHBAAAQUAAwAAAAAAAAAAAAAAAQACAxESExQy/9oACAEBAAEFAsm+u/FIVyHxlRkmWVzmoPJH/8QAFxEAAwEAAAAAAAAAAAAAAAAAARAhQf/aAAgBAwEBPwECLF//xAAWEQEBAQAAAAAAAAAAAAAAAAABEEH/2gAIAQIBAT8BWbP/xAAbEAACAgMBAAAAAAAAAAAAAAABEQAQAiFBMf/aAAgBAQAGPwIAemNVi4eVvsxRr//EABwQAAMAAgMBAAAAAAAAAAAAAAABESExEEFhUf/aAAgBAQABPyFyL4E3bX3ikPow2bTJKwVbVoLEZJ2WDSP/2gAMAwEAAgADAAAAECPfgf/EABcRAAMBAAAAAAAAAAAAAAAAAAABERD/2gAIAQMBAT8QQ6ILP//EABcRAQEBAQAAAAAAAAAAAAAAABEAARD/2gAIAQIBAT8Q2Sc5L//EABwQAQACAgMBAAAAAAAAAAAAAAEAESFRMUFh0f/aAAgBAQABPxBmYFWgMKhXG4lzGAlorcHX2K6hVQbFFxoaFQmtAV1V41xHU0lFffszIXyf/9k=&apos;); background-size: cover; display: block;&quot;&gt;&lt;/span&gt;
  &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Klara Nahrstedt&quot; title=&quot;Klara Nahrstedt&quot; src=&quot;/2021/static/63ba6c302fae1fc7b713108978be7f8c/c6c63/klara_.jpg&quot; srcset=&quot;/2021/static/63ba6c302fae1fc7b713108978be7f8c/a6b4f/klara_.jpg 200w,
/2021/static/63ba6c302fae1fc7b713108978be7f8c/c6c63/klara_.jpg 251w&quot; sizes=&quot;(max-width: 251px) 100vw, 251px&quot; style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot; loading=&quot;lazy&quot; decoding=&quot;async&quot;&gt;
  &lt;/a&gt;
    &lt;/span&gt;
    &lt;/div&gt;
    &lt;!-- - **Title**: --&gt;
    &lt;h3&gt;Klara Nahrstedt&lt;/h3&gt;
    &lt;!-- - **Date**: --&gt;
    &lt;!-- - **Abstract**:  --&gt;
    &lt;p&gt;University of Illinois at Urbana-Champaign&lt;/p&gt;
    &lt;p&gt;
        Klara Nahrstedt is the Grainger Chair in Engineering Professor in the Computer Science Department, and the Director of Coordinated Science Laboratory in the Grainger College of Engineering at the University of Illinois at Urbana-Champaign. Her research interests are directed toward tele-immersive systems, end-to-end Quality of Service (QoS), resource management in large scale distributed systems and networks, and real-time security and privacy in cyber-physical systems. She is the co-author of multimedia books “Multimedia: Computing, Communications and Applications”, published by Prentice Hall, and “Multimedia Systems”, published by Springer Verlag. She is the recipient of the IEEE Communication Society Leonard Abraham Award for Research Achievements, University Scholar, Humboldt Research Award, IEEE Computer Society Technical Achievement Award, ACM SIGMM Technical Achievement Award, TU Darmstadt Piloty Prize, the Grainger College of Engineering Drucker Award. She was the elected chair of the ACM Special Interest Group in Multimedia (SIGMM) from 2007-2013. She was the general co-chair and TPC co-chair of many international conferences including ACM Multimedia, IEEE Percom, IEEE/ACM Internet of Things Design and Implementation (IoTDI), IEEE SmartgridComm and others. Klara Nahrstedt received her Diploma in Mathematics from Humboldt University, Berlin, Germany in 1985. In 1995, she received her PhD from the University of Pennsylvania in the Department of Computer and Information Science. She is ACM Fellow, IEEE Fellow, AAAS Fellow, and Member of the German National Academy of Sciences (Leopoldina Society). 
    &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: With the emergence of new 360-degree cameras and VR/AR display devices, more diverse multimedia content has become available and with it the demand for the capability of tile-based streaming 360-degree videos to enhance users’ multimedia experience. In this talk, we will discuss the challenges of 360-degree tile-based video streaming due to its large bandwidth and low latency demands and solutions to satisfy the demands, including semantic-aware description of 360-degree videos’ viewing patterns, rate adaptation of tiled videos and view prediction techniques to enable interactive viewing via Head-Mounted Displays. Especially, we will discuss the concept of navigation graphs to &lt;strong&gt;(a).&lt;/strong&gt; capture salient objects and events as well as viewing patterns of users, and &lt;strong&gt;(b).&lt;/strong&gt; map them into efficient tile-based streaming and viewing experience. We will show how navigation graphs are serving as models to capture diverse semantic content and viewing behaviors in the temporal and spatial domains. Experimental results show that navigation graphs, provided jointly with Media Presentation Descriptors, can assist in efficient solutions of the bandwidth and latency challenges associated with view prediction and user’s interactive viewing. We will also discuss next challenges that future viewing patterns and streaming paradigms will bring as the integration of 360-degree videos, 2D/3D videos, and volumetric media in augmented reality applications is coming.&lt;/p&gt;
&lt;div class=&quot;note&quot;&gt;
    &lt;p&gt;* Joint work with Jounsup Park, Michael Zink, Ramesh Sitaraman, Qian Zhou, Bo Chen, Mingyuan Wu, John Murray, Ayush Sarkar, Eric Lee, Yinjie Zhang &lt;/p&gt;
&lt;/div&gt;
&lt;style&gt;
    .keynote-text {
        text-align: left;
    }
    .keynote-img {
        float: none;
        margin-right: 2rem;
        margin-bottom: 1rem;
        width: 250px;
    }
    .note {
        font-size: 14px;
        color: grey;
    }

    @media (min-width: 768px) {
        .keynote-text {
            text-align: justify;
        }
        .keynote-img {
            float: left;
        }
    }
&lt;/style&gt;</content:encoded></item><item><title><![CDATA[Presentation Guidelines]]></title><description><![CDATA[Paper presentations at ACM Multimedia Asia 2021 will be fully online at Zoom and Gather.Town. To provide a better experience for every…]]></description><link>https://www.acmmmasia.org/2021/presentation-guidelines/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/presentation-guidelines/</guid><pubDate>Thu, 11 Nov 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Paper presentations at ACM Multimedia Asia 2021 will be fully online at Zoom and Gather.Town. To provide a better experience for every attendance, we would like to introduce the following guidelines for presenters, session chairs, and audience respectively.&lt;/p&gt;
&lt;h2&gt;Guidelines for Presenters&lt;/h2&gt;
&lt;p&gt;All accepted papers are required to be presented online with a &lt;strong&gt;pre-filmed video and a live Q&amp;#x26;A&lt;/strong&gt;. Authors should also prepare a &lt;strong&gt;poster in PDF&lt;/strong&gt;, which will be presented in Gather.Town for online discussions.&lt;/p&gt;
&lt;p&gt;The pre-filmed video should be &lt;strong&gt;in MP4 format at 1920 x 1280p (HD)&lt;/strong&gt; that consists of the presenter’s face via webcam and the slides presented. You can use Zoom to record your presentation in the required format. PowerPoint can also help record the slides presentation as a WMV video, which can be further converted into MP4.&lt;/p&gt;
&lt;p&gt;The length of a presentation is specified as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full Regular/Applied Research: 13 minutes (playing pre-recording on Zoom) + 2 minutes Q&amp;#x26;A (Live on Zoom)&lt;/li&gt;
&lt;li&gt;Grand Challenges: 10 minutes (playing pre-recording on Zoom) + 2 minutes Q&amp;#x26;A (Live on Zoom)&lt;/li&gt;
&lt;li&gt;Workshop papers: 10 minutes (playing pre-recording on Gather.Town) + 2 minutes Q&amp;#x26;A (Live on Gather.Town)&lt;/li&gt;
&lt;li&gt;Short papers: 3 minutes lightning talk (playing pre-recording on Gather.Town)&lt;/li&gt;
&lt;li&gt;BNI papers: 3 minutes lightning talk (playing pre-recording on Gather.Town)&lt;/li&gt;
&lt;li&gt;Demo papers: 3 minutes lightning talk with a showcase of the demo (up to 5 minutes) (playing pre-recording on Gather.Town)&lt;/li&gt;
&lt;li&gt;Q&amp;#x26;A will be offered in Gather.Town for all tracks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Authors of each accepted paper need to upload their poster and pre-filmed presentation (or a link to the pre-recording for downloading, e.g., Dropbox or Google Drive) as the supplementary files to CMT (&lt;a href=&quot;https://cmt3.research.microsoft.com/MMASIA2021&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://cmt3.research.microsoft.com/MMASIA2021&lt;/em&gt;&lt;/a&gt;) by 5pm 21 Nov 2021 (AEST)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Check the time and access entry for each session from the &lt;strong&gt;&lt;a href=&quot;.&quot;&gt;&lt;em&gt;Conference Program&lt;/em&gt;&lt;/a&gt;&lt;/strong&gt;. Join your presentation session 5 - 10 minutes before the session starts. Make sure you know the time duration of the presentation and Q&amp;#x26;A.&lt;/p&gt;
&lt;h2&gt;Guidelines for Session Chairs&lt;/h2&gt;
&lt;p&gt;Join your session ahead of time to connect with the student volunteer and the speakers. This time can also be useful to test the speakers’ slides and audio.&lt;/p&gt;
&lt;p&gt;For each of the regular talks (13-minute presentation), introduce the speaker and their paper.&lt;/p&gt;
&lt;p&gt;At the end of each regular presentation, manage the Q&amp;#x26;A time (2 minutes). Questions may come from the Q&amp;#x26;A chatbox. Choose good questions from the audience for the Q&amp;#x26;A and also prepare one or two warm-up questions for each regular presentation.&lt;/p&gt;
&lt;h2&gt;Guidelines for Online Audience&lt;/h2&gt;
&lt;p&gt;For Regular/Applied Research/Grand Challenge papers, interact with presenters during the Q&amp;#x26;A time via Zoom’s chatbox.&lt;/p&gt;
&lt;p&gt;For any other papers and the posters of all accepted papers, interact with authors in Gather.Town.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Call for Workshop Papers]]></title><description><![CDATA[Workshop 1: Visual Tasks and Challenges under Low-quality Multimedia Data https://workshopcv.github.io/ Overview The field of computer…]]></description><link>https://www.acmmmasia.org/2021/call-for-workshop-papers/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/call-for-workshop-papers/</guid><pubDate>Tue, 12 Oct 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Workshop 1: Visual Tasks and Challenges under Low-quality Multimedia Data&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://workshopcv.github.io/&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://workshopcv.github.io/&lt;/em&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;The field of computer vision has been a research hotspot, and early research focused on high-quality images or daytime scenes with better illumination. Existing vision techniques have achieved better results with an approximately accuracy rate of 96% with these conditions. In practice, nearly 90% of criminal activities occur in the night scenes with low quality, especially in major cases. The video data collected by the surveillance system in these scenes has low contrast and poor quality. According to the Ministry of Public Security Evidence Identification Center (China), the proportion of poor quality video images at night is as high as 95%, and the performance of current methods on low-quality visible images is low, which is difficult to cope with the actual security needs. There is an urgent need to optimise this problem.&lt;/p&gt;
&lt;h3&gt;Challenge&lt;/h3&gt;
&lt;p&gt;The goal of this challenge is to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bring together the state of the art research on object detection under low illumination;&lt;/li&gt;
&lt;li&gt;Call for a coordinated effort to understand the opportunities and challenges emerging in object detection;&lt;/li&gt;
&lt;li&gt;Identify key tasks and evaluate the state-of-the-art methods;&lt;/li&gt;
&lt;li&gt;Showcase innovative methodologies and ideas;&lt;/li&gt;
&lt;li&gt;Introduce interesting real-world intelligent object detection under low illumination;&lt;/li&gt;
&lt;li&gt;Propose new real-world datasets and discuss future directions. We believe the workshop will offer a timely collection of research updates to benefit the researchers and practitioners working in the broad computer vision, multimedia, and pattern recognition communities.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Call for Papers&lt;/h3&gt;
&lt;p&gt;Except for the challenge, we solicit original research and survey papers in (but not limited to) the following topics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pedestrian detection in low illumination, low resolution, rain and fog, etc.&lt;/li&gt;
&lt;li&gt;Object detection in low illumination, low resolution, rain and fog, etc.&lt;/li&gt;
&lt;li&gt;Person re-identification in low illumination, low resolution, rain and fog, etc.&lt;/li&gt;
&lt;li&gt;Object recognition in low illumination, low resolution, rain and fog, etc.&lt;/li&gt;
&lt;li&gt;Segmentation in low illumination, low resolution, rain and fog, etc.&lt;/li&gt;
&lt;li&gt;Counting in low illumination, low resolution, rain and fog, etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- ### Important Dates

-	Release of Training Date: **10 August, 2021**.
-	Release of Validation Date:	**10 September, 2021**.
-	Release of Test Date: **24 September, 2021**.
-	Result Submission Close: **8 October, 2021**.
-	Workshop Paper Submission: **18 October, 2021**.
-	Workshop Notification: **1 November, 2021**. --&gt;
&lt;h3&gt;Organisers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jing Xiao&lt;/strong&gt;, (&lt;a href=&quot;mailto:jing@whu.edu.cn&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;jing@whu.edu.cn&lt;/a&gt;), Wuhan University, China&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xiao Wang&lt;/strong&gt;, (&lt;a href=&quot;mailto:hebeiwangxiao@whu.edu.cn&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;hebeiwangxiao@whu.edu.cn&lt;/a&gt;), Wuhan University, China&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Liang Liao&lt;/strong&gt;, (&lt;a href=&quot;mailto:liang@nii.ac.jp&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;liang@nii.ac.jp&lt;/a&gt;), National Institute of Informatics, Japan&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shin’ichi Satoh&lt;/strong&gt;, (&lt;a href=&quot;mailto:satoh@nii.ac.jp&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;satoh@nii.ac.jp&lt;/a&gt;), National Institute of Informatics, Japan&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Chia-wen Lin&lt;/strong&gt;, (&lt;a href=&quot;mailto:cwlin@ee.nthu.edu.tw&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;cwlin@ee.nthu.edu.tw&lt;/a&gt;), National Tsing Hua University, Taiwan&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Workshop 2: Multi-Modal Embedding and Understanding&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://mmeu.github.io/&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://mmeu.github.io/&lt;/em&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;We humans perceive the physical world via multiple ways, e.g., watching, touching, hearing, and so on, which means that we process multi-modal information for environment perception. Multi-modal understanding plays a crucial role in enabling the machine with such ability. Due to its research significance, multi-modal embedding and understanding has gained much research attention and achieved much progress in the past couple of years. The recent advances in deep learning inspire us to explore more and deeper for the multi-modal embedding and understanding, such as self-supervised learning and pre-training in it. In this workshop, we aim to bring together researchers from the field of multimedia to discuss recent research and future directions for multi-modal embedding and understanding, and their applications.&lt;/p&gt;
&lt;h3&gt;Call for Papers&lt;/h3&gt;
&lt;p&gt;Multi-modal understanding are important and fundamental problems in the field of multimodal analysis, which have been attracting much research attention in recent years. Previous works have explored shallow embedding and understanding in many downstream tasks, including cross-modal retrieval, visual navigation, VQA, visual captioning, etc. To encourage researchers to explore new and advanced techniques in this area, we are organizing a workshop on “multi-modal embedding and understanding” with the conjunction of ACM MM Asia 2021, and calling for contributions. The included (but not limited) topics are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Large-scale pre-training for multi-modal embedding and understanding&lt;/li&gt;
&lt;li&gt;Self-supervised learning in multi-modal embedding and understanding&lt;/li&gt;
&lt;li&gt;Semi-supervised learning in multi-modal embedding and understanding&lt;/li&gt;
&lt;li&gt;Contrastive learning in multi-modal embedding and understanding&lt;/li&gt;
&lt;li&gt;Interpretability in multi-modal embedding and understanding&lt;/li&gt;
&lt;li&gt;Interactive multi-modal understanding&lt;/li&gt;
&lt;li&gt;Trust AI for multi-modal understanding&lt;/li&gt;
&lt;li&gt;Cross-modal matching and retrieval&lt;/li&gt;
&lt;li&gt;Cross-modal understanding&lt;/li&gt;
&lt;li&gt;Multi-modal deep fake generation and detection&lt;/li&gt;
&lt;li&gt;And other related…&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Submission Guidelines&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; Submitted papers (.pdf format) must use the ACM Article Template &lt;a href=&quot;https://www.acm.org/publications/proceedings-template&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://www.acm.org/publications/proceedings-template&lt;/em&gt;&lt;/a&gt;. Please remember to add Concepts and Keywords.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Length:&lt;/strong&gt; Papers must be &lt;strong&gt;no longer than 6 pages&lt;/strong&gt;, including all text and figures, and up to two additional pages may be added for references. The reference pages must only contain references. Over-length papers will be rejected without review.&lt;/p&gt;
&lt;h3&gt;Workshop Schedule&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; The submission deadline is at 11:59 p.m. of the stated deadline date &lt;a href=&quot;https://www.timeanddate.com/time/zones/aoe&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Anywhere on Earth&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper Submission Deadline: &lt;strong&gt;19 October, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Notifications of Acceptance: &lt;strong&gt;1 November, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Camera-ready Submission: &lt;strong&gt;7 November, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ### Important dates
- Paper Submission Deadline: **13 October, 2021**.
- Notifications of Acceptance: **3 November, 2021**.
- Camera-ready Submission: **10 November, 2021**. --&gt;
&lt;h3&gt;Organisers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Wenguan Wang&lt;/strong&gt;, ETH Zurich, Switzerland&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xiaojun Chang&lt;/strong&gt;, RMIT, Australia&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Yanli Ji&lt;/strong&gt;, University of Electronic Science and Technology of China, China&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Yi Bin&lt;/strong&gt;, University of Electronic Science and Technology of China, China&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Workshop 3: Multi-Model Computing of Marine Big Data&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://riverw.github.io/web/MCMBD/index.html&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://riverw.github.io/web/MCMBD/index.html&lt;/em&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Different from the traditional multimedia technology which mainly focuses on human life, it is a novel and challenging problem to study multimedia data analysis methods for marine big data. Compared with traditional multimedia data, marine big data has big differences in feature distribution, content understanding, applications, etc. This makes existing multimedia analysis methods in target detection and recognition, tracking and depth estimation and other tasks cannot be simply applied to ocean data analysis. The study of multimedia data analysis technology with marine big data can help humans understand the marine, realise the detection and protection of ocean resources intelligently, and provide important technical support for the protection of various rare ocean resources.&lt;/p&gt;
&lt;h3&gt;Call for Papers&lt;/h3&gt;
&lt;p&gt;Marine multimedia data analysis and retrieval techniques are essential for marine resource exploration and marine environment prediction and forecasting. The main analytical tasks based on the marine domain include detection, identification, retrieval, tracking, and prediction forecasting of marine environmental data such as weather, temperature, humidity, and rainfall. Detection and protection of marine resources can be intelligent through detection, identification and tracking technologies, which provides important technical support for the protection of various types of rare marine resources. Today, in order to better understand the ocean, humans are rapidly collecting a wide variety of marine multimedia big data. Therefore, in this workshop, we will present the recent advances of multimedia technology in marine big data. The main analytical tasks based on the marine domain include detection, identification, retrieval, tracking, and prediction forecasting of marine environmental data such as weather, temperature, humidity, and rainfall. Exploring multi-modal data provides important technical support for understanding the marine and protecting various rare marine resources. We believe that this workshop will facilitate a closer integration of multimedia content analysis technologies with applications in the marine field. we solicit original research and survey papers in (but not limited):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Marine object detection&lt;/li&gt;
&lt;li&gt;Marine object re-identification&lt;/li&gt;
&lt;li&gt;Cross-modal hash retrieval in the marine area&lt;/li&gt;
&lt;li&gt;Fine-grained identification of marine organisms&lt;/li&gt;
&lt;li&gt;Artificial Intelligence for coastal environment evolution prediction&lt;/li&gt;
&lt;li&gt;Artificial Intelligence for optimisation of an ecological dynamic model&lt;/li&gt;
&lt;li&gt;Marine big data mining methods&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ### Important Dates


-   Submission Deadline: **15 October, 2021**.
-   Notifications of Acceptance: **15 November, 2021**.
-   Camera-ready Submission: **20 November, 2021**. --&gt;
&lt;h3&gt;Organisers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jie Nie&lt;/strong&gt;, Ocean University of China, China&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lei Huang&lt;/strong&gt;, Ocean University of China; Pilot National Laboratory for Marine Science and Technology (Qingdao)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;An-An Liu&lt;/strong&gt;, Tianjin University, China&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Junbo Guo&lt;/strong&gt;, State Key Laboratory of Communication Content Cognition, People’s Daily Online, China&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zhiqiang Wei&lt;/strong&gt;, Ocean University of China; Pilot National Laboratory for Marine Science and Technology (Qingdao)&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Student Travel Grants]]></title><description><![CDATA[Overview To promote the participation of students, ACM SIGMM and ACM MM Asia 2021 organisation committee will offer student travel grants…]]></description><link>https://www.acmmmasia.org/2021/student-travel-grants/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/student-travel-grants/</guid><pubDate>Sat, 09 Oct 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;To promote the participation of students, ACM SIGMM and ACM MM Asia 2021 organisation committee will offer student travel grants for students.&lt;/p&gt;
&lt;!-- The **Student Travel Grants** is meant to support the personal attendance of one student at the PhD School (to Shen Zhen or Gold Coast).  --&gt;
&lt;!-- **The applicant must be a SIGMM student member ([*create an account*](https://services.acm.org/public/qj/login_gensigqj.cfm?rdr=&amp;promo=QJSIG&amp;offering=044&amp;form_type=SIG)) at the time of application.** --&gt;
&lt;p&gt;&lt;strong&gt;Students must apply for a travel grant prior to the conference.&lt;/strong&gt; Successful awardees can include their registration fee, via a complimentary registration from the conference, as part of their award. To enable more students to be benefited from attending ACMMM Asia, students don’t have to have a paper at the conference in order to apply for a SIGMM travel award. The applicant must be a SIGMM student member
(&lt;a href=&quot;https://services.acm.org/public/qj/login_gensigqj.cfm?rdr=&amp;#x26;promo=QJSIG&amp;#x26;offering=044&amp;#x26;form_type=SIG&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;create an account&lt;/em&gt;&lt;/a&gt;) at the time of application.&lt;/p&gt;
&lt;p&gt;Grantees need to pay the conference registration fee and make their own travel arrangements.&lt;/p&gt;
&lt;h2&gt;Student Volunteers&lt;/h2&gt;
&lt;p&gt;Students who are awarded student travel grants might be expected to carry out light service as student volunteers during the conference.&lt;/p&gt;
&lt;h2&gt;Selection Criteria&lt;/h2&gt;
&lt;p&gt;Students being the primary authors of an accepted paper in the main conference or workshops will be given preference in the selection process.&lt;/p&gt;
&lt;p&gt;Students without papers accepted for publication at ACM MM Asia 2021 but involved in research closely related to the topics of ACMMM can also be considered.&lt;/p&gt;
&lt;p&gt;Female and minority students’ applications are especially encouraged.&lt;/p&gt;
&lt;h2&gt;Application Procedure&lt;/h2&gt;
&lt;!-- The application form could be found [*here*](https://acmsigmm.wufoo.com/forms/sigmm-student-travel-application-form/). --&gt;
&lt;ul&gt;
&lt;li&gt;Opening 4 October 2021&lt;/li&gt;
&lt;li&gt;Closing 5 November 2021&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Submit your application form via &lt;a href=&quot;https://acmsigmm.wufoo.com/forms/zs7ntks1qifyyk/&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://acmsigmm.wufoo.com/forms/zs7ntks1qifyyk/&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Contacts&lt;/h2&gt;
&lt;p&gt;For any questions regarding travel grants, please email the Student Travel Grant Chairs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gunhee Kim&lt;/strong&gt; (&lt;a href=&quot;mailto:gunhee@snu.ac.kr&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;gunhee@snu.ac.kr&lt;/a&gt;), Seoul National University, South Korea&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xinshun Xu&lt;/strong&gt; (&lt;a href=&quot;mailto:xuxinshun@sdu.edu.cn&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;xuxinshun@sdu.edu.cn&lt;/a&gt;), Shandong University, China&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Carer Awards]]></title><description><![CDATA[Overview To promote the participation of students in the online conference events and special student-oriented activities, ACM SIGMM and ACM…]]></description><link>https://www.acmmmasia.org/2021/carer-award/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/carer-award/</guid><pubDate>Sat, 09 Oct 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;To promote the participation of students in the online conference events and special student-oriented activities, ACM SIGMM and ACM MM Asia 2021 organisation committee will offer carer awards.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Carer Awards&lt;/strong&gt; is meant to support the personal attendance of one student who participates in online events of ACM MM Asia 2021. &lt;strong&gt;The applicant must be a SIGMM student member (&lt;a href=&quot;https://services.acm.org/public/qj/login_gensigqj.cfm?rdr=&amp;#x26;promo=QJSIG&amp;#x26;offering=044&amp;#x26;form_type=SIG&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;create an account&lt;/em&gt;&lt;/a&gt;) at the time of application.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Grantees need to pay the conference registration fee.&lt;/p&gt;
&lt;h2&gt;Selection Criteria&lt;/h2&gt;
&lt;p&gt;Students being the primary authors of an accepted paper in the main conference or workshops will be given preference in the selection process. Students without papers accepted for publication at ACM MM Asia 2021 but involved in research closely related to the topics of ACMMM will be considered. Female and minority students’ applications are especially encouraged.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Note: You will be required to provide the receipt and veriﬁcation for participation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Application Procedure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Opening 4 October 2021&lt;/li&gt;
&lt;li&gt;Closing 5 November 2021&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Submit your application form via &lt;a href=&quot;https://acmsigmm.wufoo.com/forms/zs7ntks1qifyyk/&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://acmsigmm.wufoo.com/forms/zs7ntks1qifyyk/&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Contacts&lt;/h2&gt;
&lt;p&gt;For any questions regarding carer awards, please email the Student Travel Grant Chairs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gunhee Kim&lt;/strong&gt; (&lt;a href=&quot;mailto:gunhee@snu.ac.kr&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;gunhee@snu.ac.kr&lt;/a&gt;), Seoul National University, South Korea&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xinshun Xu&lt;/strong&gt; (&lt;a href=&quot;mailto:xuxinshun@sdu.edu.cn&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;xuxinshun@sdu.edu.cn&lt;/a&gt;), Shandong University, China&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Call for Brave New Ideas]]></title><description><![CDATA[Overview The third ACM Multimedia Asia conference will be held in Gold Coast, Australia during 1-3 December 2021. The Brave New Ideas (BNI…]]></description><link>https://www.acmmmasia.org/2021/call-for-brave-new-ideas/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/call-for-brave-new-ideas/</guid><pubDate>Wed, 29 Sep 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;The third ACM Multimedia Asia conference will be held in Gold Coast, Australia during 1-3 December 2021. The Brave New Ideas (BNI) Track of ACM Multimedia Asia 2021 is calling for innovative papers leading to new prospects with the potential of long-lasting impact on multimedia research and practices. Submissions should be both scientifically rigorous and introduce brave and new ideas. “Brave” means that an idea has potential for high impact, providing an unconventional or out-of-the-box perspective, or fundamentally change the ways of addressing challenging problems in multimedia. “New” means that an idea has not yet been proposed. The component techniques and technologies may exist, but then their integration must be novel.&lt;/p&gt;
&lt;p&gt;In 2021, ACM Multimedia Asia covers diverse topics on multimedia research. More information is available &lt;a href=&quot;./&quot;&gt;on the homepage&lt;/a&gt;. We especially encourage the submission of interdisciplinary work crossing the boundaries of different disciplines with the help of multimedia technologies. We are also strongly looking forward to the submissions that propose novel ideas for “Multimedia interaction for online communication” to improve societal and economical activities especially during the difficult time facing the COVID-19 pandemic.&lt;/p&gt;
&lt;p&gt;The “Brave New Idea” of the paper should have scientific rigour in one or a combination of three ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;an extensive meta-review of existing work that will show that this idea can be productively pursued by the community.&lt;/li&gt;
&lt;li&gt;an extensive review of existing work that reveals a systematic shortcoming in the way that the research community has been addressing specific challenges. The BNI paper could thus propose a contrarian view.&lt;/li&gt;
&lt;li&gt;experimental validation of the paper’s BNI thesis.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that it is not necessary that papers in this track have large-scale experimental results or comparisons to the state-of-the-art, since it is expected that large, publicly available datasets may not be available, and it is unlikely that approaches already exist to which the new idea can be compared. If there is a benchmark available addressing the problem it is almost certainly not a brave new idea.&lt;/p&gt;
&lt;h2&gt;Submission Guidelines&lt;/h2&gt;
&lt;p&gt;Submissions must be 6 pages in length. For more details on the format, please see the submission instructions in the &lt;a href=&quot;./call-for-papers/&quot;&gt;main conference call for papers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Submissions will be reviewed by a set of at least three independent reviewers, and the decisions will be made on the basis of their feedback. Review criteria are: scientific impact, intellectual rigour, and a compelling case for the paper to be innovative in the sense described above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please submit your paper to the submission system &lt;a href=&quot;https://cmt3.research.microsoft.com/MMASIA2021/&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;https://cmt3.research.microsoft.com/MMASIA2021/&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We expect BNI papers to be submitted directly to the BNI track. However, at the discretion of the Program Chairs, submissions to the regular full paper track may be recommended to the “Brave New Ideas” Program Committee for consideration.&lt;/p&gt;
&lt;p&gt;Important note for the authors: The official publication date is the date the proceedings are made available in the ACM Digital Library. This date may be up to two weeks prior to the first day of the conference. The official publication date affects the deadline for any patent filings related to the published work.&lt;/p&gt;
&lt;h2&gt;Important Dates&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; The submission deadline is at 11:59 p.m. of the stated deadline date &lt;a href=&quot;https://www.timeanddate.com/time/zones/aoe&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Anywhere on Earth&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Submission deadline: &lt;strong&gt;3 October, 2021&lt;/strong&gt;. (Extended)&lt;/li&gt;
&lt;li&gt;Notification of acceptance: &lt;strong&gt;25 October, 2021&lt;/strong&gt;. (Extended)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Contacts&lt;/h2&gt;
&lt;p&gt;For any questions regarding the submission, please email the Brave New Ideas Chairs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Chee Seng Chan&lt;/strong&gt; (&lt;a href=&quot;mailto:cs.chan@um.edu.my&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;cs.chan@um.edu.my&lt;/a&gt;), University of Malaya, Malaysia&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ichiro Ide&lt;/strong&gt; (&lt;a href=&quot;mailto:ide@i.nagoya-u.ac.jp&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;ide@i.nagoya-u.ac.jp&lt;/a&gt;), Nagoya University, Japan&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Quanzeng You&lt;/strong&gt; (&lt;a href=&quot;mailto:Quanzeng.You@microsoft.com&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;Quanzeng.You@microsoft.com&lt;/a&gt;), Microsoft Cloud &amp;#x26; AI, USA&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Call for Short Papers]]></title><description><![CDATA[Overview Multimedia research focuses on technologies that enable the use and exchange of content integrating multiple perspectives of…]]></description><link>https://www.acmmmasia.org/2021/call-for-short-papers/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/call-for-short-papers/</guid><pubDate>Wed, 25 Aug 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Multimedia research focuses on technologies that enable the use and exchange of content integrating multiple perspectives of digital modalities, including images, text, video, audio (speech, music etc.), and other sensor data. ACM Multimedia Asia 2021 calls for short research papers presenting novel theoretical and algorithmic solutions addressing problems across the domain of multimedia and related applications. The conference also calls for short papers presenting novel ideas and promising (preliminary) results in realizing these ideas.&lt;/p&gt;
&lt;p&gt;Short papers provide an opportunity to describe significant novel work in progress or multimedia research. Compared to full papers, their contribution may be narrower in scope, be applied to a narrower set of application domains, or have weaker empirical support than that expected for a full paper. Submissions that are likely to generate discussions in new and emerging areas of multimedia are especially encouraged.&lt;/p&gt;
&lt;h2&gt;Expected Contents&lt;/h2&gt;
&lt;p&gt;Submissions are encouraged in all areas related to multimedia, as described in ACM MMAsia 2021 general &lt;a href=&quot;./call-for-papers&quot;&gt;&lt;em&gt;call for papers&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Submission Guidelines&lt;/h2&gt;
&lt;p&gt;The short papers should be submitted through &lt;a href=&quot;https://cmt3.research.microsoft.com/MMASIA2021/&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://cmt3.research.microsoft.com/MMASIA2021/&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Submissions will be peer-reviewed, and accepted short papers will be published in the conference proceeding. Submissions of short research papers (pdf format) must use the ACM Article Template, &lt;strong&gt;and be at most 4 pages (including figures, appendices, etc.) in length + unrestricted space for references&lt;/strong&gt;. Please remember to add &lt;a href=&quot;https://www.acm.org/publications/proceedings-template&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Concepts and Keywords&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Paper submissions must conform with the “double-blind” review policy. Please prepare your paper in a way that preserves anonymity of the authors.&lt;/p&gt;
&lt;h2&gt;Important Dates&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; The submission deadline is at 11:59 p.m. of the stated deadline date &lt;a href=&quot;https://www.timeanddate.com/time/zones/aoe&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Anywhere on Earth&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Submission deadline: &lt;strong&gt;7 September, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Decision notification: &lt;strong&gt;12 October, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Contacts&lt;/h2&gt;
&lt;p&gt;For any questions regarding submissions, please email the Short Paper Chairs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lei Zhu&lt;/strong&gt; (&lt;a href=&quot;mailto:leizhu0608@gmail.com&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;leizhu0608@gmail.com&lt;/a&gt;), Shandong Normal University, China&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zhineng Chen&lt;/strong&gt; (&lt;a href=&quot;mailto:zhinchen@fudan.edu.cn&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;zhinchen@fudan.edu.cn&lt;/a&gt;), Fudan University, China&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Min Xu&lt;/strong&gt; (&lt;a href=&quot;mailto:Min.Xu@uts.edu.au&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;Min.Xu@uts.edu.au&lt;/a&gt;), University of Technology Sydney, Australia&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Call for Demo Papers]]></title><description><![CDATA[Overview ACM Multimedia Asia is a major annual international conference to showcase the scientific achievements and industrial innovations…]]></description><link>https://www.acmmmasia.org/2021/call-for-demo-papers/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/call-for-demo-papers/</guid><pubDate>Wed, 25 Aug 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;ACM Multimedia Asia is a major annual international conference to showcase the scientific achievements and industrial innovations in the multimedia field. Multimedia research focuses on technologies that enable the use and exchange of content integrating multiple perspectives of digital modalities, including images, text, video, audio (speech, music etc.), and other sensor data.&lt;/p&gt;
&lt;p&gt;ACM Multimedia Asia 2021 will provide demonstration sessions. Demos are intended as live and interactive proof of the presenters’research ideas and technological contributions, with the goal of providing multimedia researchers and practitioners the opportunity to showcase working multimedia systems, applications, prototypes, or proof-of-concepts. Such a setting allows conference attendants to view and interact first hand with live evidence of innovative solutions and ideas in the field of multimedia and to experience leading edge research at work.&lt;/p&gt;
&lt;h2&gt;Expected Contents&lt;/h2&gt;
&lt;p&gt;Submissions are encouraged in all areas related to multimedia, as described in ACM MMAsia 2021 general &lt;a href=&quot;./call-for-papers&quot;&gt;&lt;em&gt;call for papers&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Facilities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Once accepted, demonstration presenters will be provided with a table, poster board, power outlet and wireless (shared) Internet. Demo presenters are expected to bring with themselves everything else needed for their demo presentations, such as hardware, laptops, sensors, PCs, etc. However if you have special requests such as larger space, special lighting conditions and so on, we will do our best to arrange them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Important note for the authors:&lt;/strong&gt; The official publication date is the date the proceedings are made available in the ACM Digital Library. This date may be up to two weeks prior to the first day of the conference. The official publication date may affect the deadline for any patent filings related to published work.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Submission Guidelines&lt;/h2&gt;
&lt;p&gt;The demo papers should be submitted through &lt;a href=&quot;https://cmt3.research.microsoft.com/MMASIA2021/&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://cmt3.research.microsoft.com/MMASIA2021/&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All submissions will be peer-reviewed, and the demos will be selected through a jury process. Please submit a two page demo proposals (excluding references) using the ACM Article Template. Please remember to add &lt;a href=&quot;https://www.acm.org/publications/proceedings-template&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Concepts and Keywords&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Please note that the submission is NOT blind, there is no need to anonymise presenters . Each demo submission must contain a supplementary file, either in PowerPoint format (no more than 10 slides) or in video (no longer than 3 minutes) explaining to the jury:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the scientific or engineering concept behind the work?&lt;/li&gt;
&lt;li&gt;What is the novelty of the work and how is the work different from existing systems or techniques?&lt;/li&gt;
&lt;li&gt;What will be actually shown during the demo?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, a PDF document containing one page description about your demo booth configuration may be provided if you have special requests. Please note that the submission system accepts only one supplementary file (Max 10MB). Therefore you have to compress multiple files into one ZIP file for submitting the supplementary materials. If the total size of your supplementary materials exceeds 10MB, please contact the demo chairs to arrange a way to submit the file.&lt;/p&gt;
&lt;h2&gt;Important Dates&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; The submission deadline is at 11:59 p.m. of the stated deadline date &lt;a href=&quot;https://www.timeanddate.com/time/zones/aoe&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Anywhere on Earth&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Submission deadline: &lt;strong&gt;7 September, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Decision notification: &lt;strong&gt;12 October, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Contacts&lt;/h2&gt;
&lt;p&gt;For any questions regarding submissions, please email the Demo Chairs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lei Zhu&lt;/strong&gt; (&lt;a href=&quot;mailto:leizhu0608@gmail.com&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;leizhu0608@gmail.com&lt;/a&gt;), Shandong Normal University, China&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zhineng Chen&lt;/strong&gt; (&lt;a href=&quot;mailto:zhinchen@fudan.edu.cn&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;zhinchen@fudan.edu.cn&lt;/a&gt;), Fudan University, China&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Min Xu&lt;/strong&gt; (&lt;a href=&quot;mailto:Min.Xu@uts.edu.au&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;Min.Xu@uts.edu.au&lt;/a&gt;), University of Technology Sydney, Australia&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Call for Applied Research Track Papers]]></title><description><![CDATA[Overview ACM Multimedia Asia 2021 invites the submission of research papers presenting novel theoretical and algorithmic solutions…]]></description><link>https://www.acmmmasia.org/2021/call-for-applied-research-track-papers/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/call-for-applied-research-track-papers/</guid><pubDate>Wed, 25 Aug 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;ACM Multimedia Asia 2021 invites the submission of research papers presenting novel theoretical and algorithmic solutions addressing problems across the domain of multimedia and related applications.&lt;/p&gt;
&lt;p&gt;The Applied Research Track of ACM Multimedia Asia 2021 covers all aspects of innovative commercial or industrial-strength multimedia algorithms, systems, solutions, and applications, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Content-based image/video indexing/retrieval/recommendation&lt;/li&gt;
&lt;li&gt;Cross-domain person re-identification&lt;/li&gt;
&lt;li&gt;Object detection/tracking/recognition/segmentation&lt;/li&gt;
&lt;li&gt;3D reconstruction&lt;/li&gt;
&lt;li&gt;Visual localisation/navigation&lt;/li&gt;
&lt;li&gt;Social computing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All accepted papers will appear in the proceeding.&lt;/p&gt;
&lt;h2&gt;Submission Guidelines&lt;/h2&gt;
&lt;p&gt;Papers should be submitted through the conference submission system by selecting the &lt;strong&gt;“Applied Research” track&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Applied research track papers will go through the same double-blind review process as the main track but without a rebuttal phase due to time limitations. Qualified boundary papers rejected by the main track are encouraged to resubmit to the Applied Research Track if the issues raised by the reviewers are sufficiently addressed (please indicate with a cover letter). Submissions must be &lt;strong&gt;6 pages&lt;/strong&gt; in length and the paper format is the same as the main track.&lt;/p&gt;
&lt;p&gt;For more details on the format, please see the submission instructions in the main conference &lt;a href=&quot;./call-for-papers&quot;&gt;Call for Regular Papers&lt;/a&gt;.&lt;/p&gt;
&lt;!-- **Please submit your paper to the submission system [https://cmt3.research.microsoft.com/MMASIA2021/](https://cmt3.research.microsoft.com/MMASIA2021/)** --&gt;
&lt;h2&gt;Important Dates&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; The submission deadline is at 11:59 p.m. of the stated deadline date &lt;a href=&quot;https://www.timeanddate.com/time/zones/aoe&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Anywhere on Earth&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Submission deadline: &lt;strong&gt;1 October, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Notification of acceptance: &lt;strong&gt;15 October, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Contacts&lt;/h2&gt;
&lt;p&gt;For any questions regarding submissions, please email the Applied Research Track Chairs (&lt;a href=&quot;mailto:mmasia21-app@hotmail.com&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;mmasia21-app@hotmail.com&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Yong Man Ro&lt;/strong&gt;, Korea Advanced Institute of Science and Technology, South Korea&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Chuang Gan&lt;/strong&gt;, MIT-IBM Watson AI Lab, USA&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Multimedia Grand Challenges]]></title><description><![CDATA[Challenge 1: Multi-modal Video Understanding in a Noisy Environment https://midas-research.github.io/noisy-mmvu/ Challenge Overview In this…]]></description><link>https://www.acmmmasia.org/2021/call-for-grand-challenge-submissions/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/call-for-grand-challenge-submissions/</guid><pubDate>Sat, 07 Aug 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Challenge 1: Multi-modal Video Understanding in a Noisy Environment&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://midas-research.github.io/noisy-mmvu/&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://midas-research.github.io/noisy-mmvu/&lt;/em&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Challenge Overview&lt;/h3&gt;
&lt;p&gt;In this challenge, we present a novel but challenging task of video understanding in a multi- modal noisy environment. While image based object detection and recognition has improved significantly in the last decade, the same has not been replicated in the video domain. Unlike images, understanding videos has proven to be difficult due to the added complexities of extra dimension and complicated sequence of motions. One of the major obstacle in learning better techniques that can understand complex actions in videos is the sheer lack of large scale annotated data. Large annotated datasets such as ImageNet and Open Image have propelled research in image understanding. The issue with doing this for videos is the massive cost of annotating millions of videos, which is why only a handful of large scale video datasets have been produced such as Kinetics 400/600/700, AVA and Youtube8M. An alternative to annotating such large video datasets is to accumulate the data from the web using specific search queries. However, this automatic annotation comes at a cost of variable noise in the data and annotation. As such, there is an ever growing need to generate better techniques for video action understanding based on such noisy datasets.&lt;/p&gt;
&lt;h3&gt;Task Descriptions&lt;/h3&gt;
&lt;p&gt;In this grand challenge we propose three different tasks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Task 1: Multi-modal Noisy Learning for Action Recognition&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Task 2: Multi-modal Noisy Learning for Video Retrieval&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Task 3: Multi-modal Noisy Video Understanding&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;About The Dataset&lt;/h3&gt;
&lt;p&gt;We are releasing two splits: 100k and 25k, along with its meta data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full Split: &lt;a href=&quot;https://drive.google.com/file/d/1Zj0Lf4JYUc_8pnHyKe23ctAWWn3JCR9q/view&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Metadata&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://drive.google.com/file/d/12Hc1bUrTxzB9mKDwJ2bT4HFtx4AqKucZ/view&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Tags&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;100k Split: &lt;a href=&quot;https://drive.google.com/file/d/19_ddJZkgTVDNC2Hwp4ZWKhV4hfF2N6zv/view&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Videos&lt;/em&gt;&lt;/a&gt; (Compressed Size:1.3TB , Uncompressed Size: 1.4TB), &lt;a href=&quot;https://drive.google.com/file/d/1f81Q-N7DY21wFenS1ThbgO7kWpWGJseb/view&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Metadata&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://drive.google.com/file/d/11uSVC3dw9Om7bT25hC_pqh7WPNRBQMXW/view&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Tags&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://drive.google.com/file/d/19qIUb4iOprVm-M3cI7d_5fJzTnzArepO/view&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Features&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;25k Split: &lt;a href=&quot;https://drive.google.com/file/d/1frvv3JXRoiTn7hubMTl5BQDdVIcY9XO9/view&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Videos&lt;/em&gt;&lt;/a&gt; (Compressed Size: 420GB, Uncompressed Size: 425GB), &lt;a href=&quot;https://drive.google.com/file/d/1y_QP0Vm4KKCaTSTiY3MrX347Fs14kZ1o/view&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Metadata&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://drive.google.com/file/d/1grVNqoR1MobJe0vWYe77zSdbdyYVvHMP/view&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Tags&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://drive.google.com/file/d/1gpwLppZ_noSHxFTXvXDH1zWK27q-7e2z/view&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Features&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Please note:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;For the full split of 2.4M, participants will have to download the videos themselves from Flickr.&lt;/li&gt;
&lt;li&gt;Tags json file contains tags associated with each video, as the main metadata file only contains tag IDs.&lt;/li&gt;
&lt;li&gt;Files are compressed using bzip2.&lt;/li&gt;
&lt;/ul&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;hr&gt;
&lt;h2&gt;Challenge 2: Deep Video Understanding&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://sites.google.com/view/dvu-asia-challenge-2021&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://sites.google.com/view/dvu-asia-challenge-2021&lt;/em&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Deep video understanding is a difficult task which requires systems to develop a deep analysis and understanding of the relationships between different entities in video, to use known information to reason about other, more hidden information, and to populate a knowledge graph (KG) with all acquired information. To work on this task, a system should take into consideration all available modalities (speech, image/video, and in some cases text). The aim of this new challenge is to push the limits of multimodal extraction, fusion, and analysis techniques to address the problem of analyzing long duration videos holistically and extracting useful knowledge to utilise it in solving different types of queries. The target knowledge includes both visual and non-visual elements. As videos and multimedia data are getting more and more popular and usable by users in different domains, the research, approaches and techniques we aim to be applied in this Grand Challenge will be very relevant in the coming years and near future.&lt;/p&gt;
&lt;h3&gt;Challenge Overview&lt;/h3&gt;
&lt;p&gt;Interested participants are invited to apply their approaches and methods on an extended novel Deep Video Understanding (DVU) dataset being made available by the challenge organisers. This includes total of 14 movies with a Creative Commons license. The dataset are annotated by human assessors and final ground truth, both at the overall movie level (Ontology of relations, entities, actions &amp;#x26; events, Knowledge Graph, and names and images of all main characters), and the individual scene level (Ontology of relationships, interactions, locations, scene sentiments, character emotional states, and scene textual summaries) will be provided for 70% of the dataset to participating researchers for training and development of their systems. The organisers will support evaluation and scoring for a hybrid of main query types, at the overall movie level and at the individual scene level distributed with the dataset (please refer to the &lt;a href=&quot;(https://sites.google.com/view/dvu-asia-challenge-2021/home/supported-datasets)&quot;&gt;&lt;em&gt;dataset webpage&lt;/em&gt;&lt;/a&gt; for more details):&lt;/p&gt;
&lt;h4&gt;Example Question types at Overall Movie Level:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Multiple choice question answering on part of Knowledge Graph for selected movies.&lt;/li&gt;
&lt;li&gt;Possible path analysis between persons / entities of interest in a Knowledge Graph extracted from selected movies.&lt;/li&gt;
&lt;li&gt;Fill in the Graph Space - Given a partial graph, systems will be asked to fill in the graph space.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Example Question types at Individual Scene Level:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Find next or previous interaction, given two people, a specific scene, and the interaction between them.&lt;/li&gt;
&lt;li&gt;Find a unique scene given a set of interactions and a scene list.&lt;/li&gt;
&lt;li&gt;Fill in the Graph Space - Given a partial graph for a scene, systems will be asked to fill in the graph space.&lt;/li&gt;
&lt;li&gt;Match between selected scenes and set of scene descriptions written in natural language.&lt;/li&gt;
&lt;li&gt;Classify scene sentiment.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- **For more information regarding this topic**, please refer to the link: [*https://sites.google.com/view/dvu-asia-challenge-2021*](https://sites.google.com/view/dvu-asia-challenge-2021). --&gt;</content:encoded></item><item><title><![CDATA[Important Dates]]></title><description><![CDATA[The submission deadline is at 11:59 p.m. of the stated deadline date Anywhere on Earth.]]></description><link>https://www.acmmmasia.org/2021/important-dates/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/important-dates/</guid><pubDate>Sun, 25 Jul 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;The submission deadline is at 11:59 p.m. of the stated deadline date &lt;a href=&quot;https://www.timeanddate.com/time/zones/aoe&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Anywhere on Earth&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;!-- you can also write HTML codes in markdowns! --&gt;
&lt;div class=&quot;pt-3&quot; style=&quot;max-width: 700px;&quot;&gt;
&lt;div class=&quot;wrapper&quot;&gt;
        &lt;div&gt;Workshop/Grand Challenge Proposals Submission:&lt;/div&gt;&lt;div&gt;&lt;/div&gt; 
        &lt;div&gt;5 July, 2021&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
        &lt;div&gt;Workshop/Grand Challenge Proposals Notification:&lt;/div&gt;&lt;div&gt;&lt;/div&gt; 
        &lt;div&gt;18 July, 2021&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
        &lt;div style=&quot;color: #51247a&quot;&gt;&lt;strong&gt;Regular Papers Submission (Full Track):&lt;/strong&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;
        &lt;div style=&quot;color: #51247a&quot;&gt;&lt;strong&gt;21 Aug, 2021&lt;/strong&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
        &lt;div&gt;Tutorial Proposals Submission:&lt;/div&gt;&lt;div&gt;&lt;/div&gt;
        &lt;div&gt;30 Aug, 2021&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
        &lt;div&gt;Short/Demo Papers Submission:&lt;/div&gt;&lt;div&gt;&lt;/div&gt;
        &lt;div&gt;7 Sep, 2021&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
        &lt;div&gt;Tutorial Proposals Notification:&lt;/div&gt;&lt;div&gt;&lt;/div&gt;
        &lt;div&gt;15 Sep, 2021&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
        &lt;div style=&quot;color: #51247a&quot;&gt;&lt;strong&gt;Regular Papers Notification (Full Track):&lt;/strong&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;
        &lt;div style=&quot;color: #51247a&quot;&gt;&lt;strong&gt;27 Sep, 2021&lt;/strong&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
        &lt;div&gt;Workshop Papers Submission:&lt;/div&gt;&lt;div&gt;&lt;/div&gt;
        &lt;div&gt;Deadlines Vary&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
        &lt;div&gt;Grand Challenge/Applied Research Track Papers Submission:&lt;/div&gt;&lt;div&gt;&lt;/div&gt;
        &lt;div&gt;1 Oct, 2021&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
        &lt;div&gt;Brave New Idea Papers Submission (Extended):&lt;/div&gt;&lt;div&gt;&lt;/div&gt;
        &lt;div&gt;3 Oct, 2021&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
        &lt;div&gt;Short/Demo Papers Notification:&lt;/div&gt;&lt;div&gt;&lt;/div&gt;
        &lt;div&gt;12 Oct, 2021&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
        &lt;div&gt;Grand Challenge/Applied Research Track Papers Notification:&lt;/div&gt;&lt;div&gt;&lt;/div&gt;
        &lt;div&gt;15 Oct, 2021&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
        &lt;div&gt;Brave New Idea Papers Notification (Extended):&lt;/div&gt;&lt;div&gt;&lt;/div&gt;
        &lt;div&gt;25 Oct, 2021&lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Call for Regular Papers]]></title><description><![CDATA[Overview ACM Multimedia Asia 2021 invites submission of research papers presenting novel theoretical and algorithmic solutions addressing…]]></description><link>https://www.acmmmasia.org/2021/call-for-papers/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/call-for-papers/</guid><pubDate>Fri, 23 Jul 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;ACM Multimedia Asia 2021 invites submission of research papers presenting novel theoretical and algorithmic solutions addressing problems across the domain of multimedia and related applications. The conference also encourages visionary papers on new and emerging topics; papers presenting novel ideas with promising (preliminary) results in realizing these ideas; application-oriented papers that make innovative technical contributions to social good, healthcare, etc. The topics of interest include but are not limited to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multimedia and Vision&lt;/li&gt;
&lt;li&gt;Multimodal Analysis and Description&lt;/li&gt;
&lt;li&gt;Deep Learning for Multimedia&lt;/li&gt;
&lt;li&gt;Emotional and Social Signals in Multimedia&lt;/li&gt;
&lt;li&gt;Multimedia Search and Recommendation&lt;/li&gt;
&lt;li&gt;Social Multimedia&lt;/li&gt;
&lt;li&gt;Multimedia HCI and Quality of Experience&lt;/li&gt;
&lt;li&gt;Multimedia Art, Entertainment and Culture&lt;/li&gt;
&lt;li&gt;Music and Audio Processing in Multimedia&lt;/li&gt;
&lt;li&gt;Mobile Multimedia&lt;/li&gt;
&lt;li&gt;Multimedia Systems&lt;/li&gt;
&lt;li&gt;Multimedia Database&lt;/li&gt;
&lt;li&gt;Multimedia Transport and Delivery&lt;/li&gt;
&lt;li&gt;Multimedia for Collaboration in Education&lt;/li&gt;
&lt;li&gt;Multimedia Virtual/Augmented Reality&lt;/li&gt;
&lt;li&gt;Multimedia for Social Goods&lt;/li&gt;
&lt;li&gt;Multimedia for HealthCare&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;All the best paper candidates of ACM Multimedia Asia 2021 will be recommended to the ACM TOMM Special Issue on Extended Best Paper Candidates of ACM MM Asia 2021. Moreover, the selected high-quality papers will also be recommended to the related special issues on IEEE TCSVT and ACM TOMM.&lt;/strong&gt;&lt;/p&gt;
&lt;!-- **The related special issues:**

- **ACM TOMM Special Issue on Extended Best Paper Candidates of ACM MM Asia 2021**
- **IEEE T-CSVT Special Issue on Visual Analysis and Understanding under Adverse Environments**
- **ACM TOMM Special Issue on Trustworthy Multimedia Computing and Applications in Urban Sense** --&gt;
&lt;h2&gt;Submission Guidelines&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Submission Site:&lt;/strong&gt; &lt;a href=&quot;https://cmt3.research.microsoft.com/MMASIA2021/Submission/Index&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://cmt3.research.microsoft.com/MMASIA2021/Submission/Index&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paper format:&lt;/strong&gt; Submitted papers (.pdf format) must use the &lt;a href=&quot;https://www.acm.org/publications/proceedings-template&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;ACM Article Template&lt;/em&gt;&lt;/a&gt;. Please remember to add Concepts and Keywords. Please use the template in traditional &lt;strong&gt;double-column&lt;/strong&gt; format to prepare your submissions. For example, word users may use Word Interim Template, and latex users may use sample-sigconf template.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Length:&lt;/strong&gt; Papers must be no longer than 6 pages, including all text and figures. There is no limit to the pages of references.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Blinding:&lt;/strong&gt; Paper submissions must conform with the “double-blind” review policy. This means that the authors should not know the names of the reviewers of their papers, and reviewers should not know the names of the authors. Please prepare your paper in a way that preserves anonymity of the authors.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do not put the authors’ names under the title.&lt;/li&gt;
&lt;li&gt;Avoid using phrases such as “our previous work” when referring to earlier publications by the authors.&lt;/li&gt;
&lt;li&gt;Remove information that may identify the authors in the acknowledgments (e.g., co-workers and grant IDs).&lt;/li&gt;
&lt;li&gt;Check supplemental material (e.g., titles in the video clips, or supplementary documents) for information that may identify the authors’ identities.&lt;/li&gt;
&lt;li&gt;Avoid providing links to websites that identify the authors.&lt;/li&gt;
&lt;li&gt;Papers without appropriate blinding will be desk rejected without review.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Originality:&lt;/strong&gt; Papers submitted to ACM Multimedia Asia 2021 must be the original work of the authors. The may not be simultaneously under review elsewhere. Publications that have been peer-reviewed and have appeared at other conferences or workshops may not be submitted to ACM Multimedia Asia 2021(see also the arXiv/Archive policy below). Authors should be aware that ACM has a strict policy with regard to &lt;a href=&quot;https://www.acm.org/publications/policies/plagiarism&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;plagiarism and self-plagiarism&lt;/em&gt;&lt;/a&gt;. The authors’ prior work must be cited appropriately.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Author list:&lt;/strong&gt; Please ensure that you submit your papers with the full and final list of authors in the correct order. The author list registered for each submission is not allowed to change in any way after the paper submission deadline. (Note that this rule regards the identity of authors, e.g., typos are correctable.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proofreading:&lt;/strong&gt; Please proofread your submission carefully. It is essential that the language used in the paper is clear and correct so that it is easily understandable. (Either US English or UK English spelling conventions are acceptable.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ArXiv/archive policy:&lt;/strong&gt; In accordance with ACM guidelines, all SIGMM-sponsored conferences adhere to the following policy regarding arXiv papers:&lt;/p&gt;
&lt;p&gt;We define a publication as a written piece documenting scientific work that was submitted for review by peers for either acceptance or rejection, and, after review, has been accepted. Documentation of scientific work that is published in a not-for-profit archive without any form of peer-review (departmental Technical Report, arXiv.org, etc.) is not considered a publication. However, this definition of publication does include peer-reviewed workshop papers, even if they do not appear in formal proceedings. Any submission to ACM Multimedia must not have substantial overlap with prior publications or other work currently undergoing peer review anywhere.&lt;/p&gt;
&lt;p&gt;Note that documents published on website archives are subject to change. Citing such documents is discouraged. Furthermore, ACM Multimedia will review the documents formally submitted and any additional information in a web archive version will not affect the review.&lt;/p&gt;
&lt;h2&gt;Important Dates&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; The submission deadline is at 11:59 p.m. of the stated deadline date &lt;a href=&quot;https://www.timeanddate.com/time/zones/aoe&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Anywhere on Earth&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Submission deadline: &lt;strong&gt;12 August, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Decision notification: &lt;strong&gt;27 September, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Review and Rebuttal Process&lt;/h2&gt;
&lt;p&gt;Each submission will be reviewed by at least three reviewers, adhering to the &lt;a href=&quot;./reviewer-guidelines&quot;&gt;&lt;em&gt;reviewer guidelines&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After receiving the reviews, authors may optionally submit a rebuttal to address the reviewers’ comments as plain text in the cmt3 interface. There is a 500 word limit for this.&lt;/p&gt;
&lt;p&gt;Note that the author rebuttal is optional, it is meant to provide you with an opportunity to rebut factual errors or to supply additional information requested by the reviewers. It is NOT intended to add new contributions (theorems, algorithms, experiments) that were not included in the original submission and were not requested by the reviewers.&lt;/p&gt;
&lt;p&gt;Authors may optionally contact the Author’s Advocate, whose role is to listen to the authors, and to help them if reviews are clearly below average quality. The Author’s Advocate operates independently from the Technical Program Committee.&lt;/p&gt;
&lt;h2&gt;Publication&lt;/h2&gt;
&lt;p&gt;The conference proceedings will be published in the ACM Digital Library. The official publication date is the date the proceedings are made available in the ACM Digital Library. This date may be up to two weeks prior to the first day of the conference. The official publication date affects the deadline for any patent filings related to the published work.&lt;/p&gt;
&lt;p&gt;Please download the PDF version of the ACM MM Asia 2021 flyer &lt;a href=&quot;./uploads/flyer.pdf&quot;&gt;&lt;em&gt;here&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Reviewer Guidelines]]></title><description><![CDATA[Overview Note: These guidelines may be subject to minor revisions before the submission deadline. Thank you for your work reviewing for ACM…]]></description><link>https://www.acmmmasia.org/2021/reviewer-guidelines/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/reviewer-guidelines/</guid><pubDate>Thu, 15 Jul 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Note: These guidelines may be subject to minor revisions before the submission deadline.&lt;/p&gt;
&lt;p&gt;Thank you for your work reviewing for ACM Multimedia Asia 2021. We appreciate your service. Your time and effort directly contributes to maintaining the high quality of the conference and strengthening the multimedia research community.&lt;/p&gt;
&lt;p&gt;As a Technical Program Committee (TPC) member, we expect that you are already experienced with writing excellent reviews. However, in practice we find that guidelines can help streamline the process. ACM Multimedia Asia will be announcing awards for the best reviewers of the conference. The guidelines also serves as a basis for the best reviewer decisions.&lt;/p&gt;
&lt;p&gt;As an Area Chair (AC), we expect that you have experience summarizing the strengths and weaknesses of papers pointed out by reviewers, and making recommendations. You will want to be thoroughly familiar with the review guidelines. For completeness, at the bottom of the guidelines the responsibilities of an AC are summarised.&lt;/p&gt;
&lt;p&gt;The Golden Rule of reviewing: Write a review that you would like to receive yourself.&lt;/p&gt;
&lt;p&gt;A review should be helpful to the authors, even if the review recommends rejection of the paper.&lt;/p&gt;
&lt;p&gt;The reviews are anonymous, but please make sure that you deliver your best work, and that you write reviews that you would be proud to associate with your name.&lt;/p&gt;
&lt;h2&gt;Best practices for reviewing&lt;/h2&gt;
&lt;h4&gt;Check the paper topic:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Confirm that the paper that you are reviewing falls into the topical scope of ACM Multimedia Asia, as defined by the Call for Regular Papers. Re-read the Call for Regular Papers to make sure that you have this year’s themes in the forefront of your mind while you are reviewing.&lt;/li&gt;
&lt;li&gt;Remember that the problem addressed by an ACM Multimedia Asia paper is expected to involve more than a single modality, or is expected to be related to the challenge of how people interpret and use multimedia. Papers that address a single modality only and also fail to contribute new knowledge on human use of multimedia must be rejected as out of scope for the conference.&lt;/li&gt;
&lt;li&gt;Although many submissions to ACM Multimedia Asia make a technical contribution in the form of a new algorithm, not all do, nor is it a requirement of ACM Multimedia Asia. Do not give less value to papers that carry out studies of new multimedia problems because they do not make a novel algorithmic contribution. Instead, judge these papers by the novelty of their insights and the value these insights could have for the community.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Support your statements:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Reviews should not just state, “It is well known that…”, but rather, they should include citations.&lt;/li&gt;
&lt;li&gt;Reviews should not just state, “Important references are missing…”, but rather, they should include examples; Reviewers should list their own references only in very rare cases that these are indeed the most relevant references for the authors to refer to.&lt;/li&gt;
&lt;li&gt;Reviews should not just state, “Authors should compare to the state of the art…”, but rather, they should cite specific work (i.e., peer-reviewed references) that they feel the authors should have considered and why.&lt;/li&gt;
&lt;li&gt;Authors appreciate if reviewers are generous with their feedback.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Respect the authors:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Reviews should critique “the paper”, and not the authors.&lt;/li&gt;
&lt;li&gt;Reviews should try not address the authors directly, esp. not as “you”. (A direct address can be interpreted as an affront by the reader).&lt;/li&gt;
&lt;li&gt;During the review process, no attempt should be made to guess the identity of the authors. (If you discover it by accident, please complete your review, but notify your AC.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Please include in your review:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Statement of novelty: What does the paper contribute? Is that contribution valuable for the multimedia research community? Does the paper cover all the relevant related work, and explain how its contribution builds on the related work?&lt;/li&gt;
&lt;li&gt;Statement of scientific rigor: Are the experiments well designed? Are the experiments sufficient to support the claims made by the paper? Are they reproducible? Have the authors released a resource, such as a data set or code?&lt;/li&gt;
&lt;li&gt;Fixes that the authors should make for the camera ready. We can trust the authors to correct minor errors. Authors generally also will state their commitment to correcting minor errors found during the review process during the rebuttal. However, major flaws must lead to rejection, since it is not possible to confirm that the authors have actually corrected major flaws successfully (i.e., the paper does not go back to the reviewers for checking).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Ensuring review quality:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;When you finish a review, and before you submit it, please check it over to make sure that it follows these guidelines. Checking your review is good practice and will also save the ACs the effort of chasing you.&lt;/li&gt;
&lt;li&gt;Note that high-quality, accurate reviews will also ensure that the authors do not request your review to be referred to the Authors’ Advocate.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Policy on arXiv papers:&lt;/h4&gt;
&lt;p&gt;We consider a “publication” to be a manuscript that has undergone peer review and has been accepted for publication. This means that the following points apply to arXiv papers (and any other papers available online that have not been peer reviewed):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the paper that you are reviewing is available on arXiv, and has not been published elsewhere, it is an acceptable submission to ACM Multimedia Asia, since arXiv papers are not peer reviewed and are not publications;&lt;/li&gt;
&lt;li&gt;Please do not insist that the authors cite a paper that is only on arXiv and has not otherwise been published. Since arXiv papers are not all peer-reviewed publications, missing an arXiv paper does &lt;em&gt;not&lt;/em&gt; count as missing related work;&lt;/li&gt;
&lt;li&gt;Likewise, if the authors do not compare their work with an approach described in an arXiv paper, it does &lt;em&gt;not&lt;/em&gt; count as a weakness in their experimental evaluation of their own approach;&lt;/li&gt;
&lt;li&gt;If you know of an interesting arXiv paper relevant to the paper you are reviewing, you are more than welcome to tell the authors about it, but make sure you mark the reference as FYI “for your information” so that the authors know that you do not regard it as missing related work.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Responsibilities of Area Chairs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;An Area Chair solicits reviews for up to 10 papers.&lt;/li&gt;
&lt;li&gt;An Area Chair reads the ten papers and the reviews.&lt;/li&gt;
&lt;li&gt;An Area Chair ensures that the reviews have followed the reviewer guidelines.&lt;/li&gt;
&lt;li&gt;An Area Chair summarises the strengths and weaknesses of each paper, makes a recommendation, and presents the recommendation at the TPC meeting.&lt;/li&gt;
&lt;li&gt;An Area Chair adheres to the schedule for the review and selection process.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you have any questions about the guidelines, please contact the Program Chairs.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Call for Workshops]]></title><description><![CDATA[Call for Workshop Proposals We are soliciting proposals for workshops to be held in conjunction with the ACM Multimedia Asia 2021. The…]]></description><link>https://www.acmmmasia.org/2021/call-for-workshops/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/call-for-workshops/</guid><pubDate>Thu, 15 Jul 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Call for Workshop Proposals&lt;/h2&gt;
&lt;p&gt;We are soliciting proposals for workshops to be held in conjunction with the ACM Multimedia Asia 2021. The purpose of the workshop is to provide a comprehensive forum on current and emerging topics that could not be fully explored during the main conference and to encourage in-depth discussion of technical and application issues.&lt;/p&gt;
&lt;h2&gt;Proposal Format&lt;/h2&gt;
&lt;p&gt;Each workshop proposal (maximum 4 pages, in PDF format) must include:&lt;/p&gt;
&lt;!-- need to use html syntax for lists with sublists --&gt;
&lt;ol class=&quot;list-bold&quot;&gt;
   &lt;li&gt;&lt;strong&gt;Title of the workshop.&lt;/strong&gt;&lt;/li&gt;
   &lt;li&gt;&lt;strong&gt;Workshop organisers (name, contact and short biography).&lt;/strong&gt;&lt;/li&gt;
   &lt;li&gt;&lt;strong&gt;Scope and topics of the workshop.&lt;/strong&gt;&lt;/li&gt;
   &lt;li&gt;&lt;strong&gt;Rationale:&lt;/strong&gt;
      &lt;ul&gt;
         &lt;li&gt;Why the workshop is related to ACM Multimedia Asia 2021.&lt;/li&gt;
         &lt;li&gt;Why the topic is important.&lt;/li&gt;
         &lt;li&gt;Why the workshop may attract a reasonable number of attendees.&lt;/li&gt;
         &lt;li&gt;A brief biography for each organiser and panelist.&lt;/li&gt;
      &lt;/ul&gt;
   &lt;/li&gt;
   &lt;li&gt;&lt;strong&gt;Workshop details:&lt;/strong&gt;
      &lt;ul&gt;
         &lt;li&gt;A draft call for papers (including organisers, program committee, and steering committee if any). Organisers are expected to be fully committed and physically present at the workshop.&lt;/li&gt;
         &lt;li&gt;Workshop tentative schedule (number of expected papers, duration full-half day, format talks/posters, etc.). We encourage both full-day and half-day events that demonstrate the interest of the community in the proposed topic and guarantee the commitment of the organisers.&lt;/li&gt;
         &lt;li&gt;Names of potential participants and invited speakers (if any).&lt;/li&gt;
      &lt;/ul&gt;
   &lt;/li&gt;
   &lt;li&gt;&lt;strong&gt;Workshop history:&lt;/strong&gt;
      &lt;ul&gt;
         &lt;li&gt;If there are past workshops, the history of the workshop.&lt;/li&gt;
      &lt;/ul&gt;
   &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Important Dates&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; The submission deadline is at 11:59 p.m. of the stated deadline date &lt;a href=&quot;https://www.timeanddate.com/time/zones/aoe&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Anywhere on Earth&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Workshop proposal submission: &lt;strong&gt;5 July, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Decision notification: &lt;strong&gt;18 July, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Contacts&lt;/h2&gt;
&lt;p&gt;For questions regarding the submission, you can email the workshop chairs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Yanli Ji&lt;/strong&gt; (&lt;a href=&quot;mailto:yanliji@uestc.edu.cn&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;yanliji@uestc.edu.cn&lt;/a&gt;), University of Electronic Science and Technology of China&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zheng Wang&lt;/strong&gt; (&lt;a href=&quot;mailto:wangz@nii.ac.jp&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;wangz@nii.ac.jp&lt;/a&gt;), The University of Tokyo, Japan&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Call for PhD School Participants]]></title><description><![CDATA[Overview ACM Multimedia Asia 2021 will provide Twin-City PhD School (a special session of the main conference). Who: PhD students and ECRs…]]></description><link>https://www.acmmmasia.org/2021/call-for-phd-school-participants/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/call-for-phd-school-participants/</guid><pubDate>Thu, 15 Jul 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;ACM Multimedia Asia 2021 will provide Twin-City PhD School (a special session of the main conference).&lt;/p&gt;
&lt;h4&gt;Who:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;PhD students and ECRs&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Where:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Australia: Gold Coast – Griffith University&lt;/li&gt;
&lt;li&gt;China: Shenzhen – Peng Cheng Laboratory&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Why:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Sharing research experiences and skills for research capability enhancement and career development;&lt;/li&gt;
&lt;li&gt;A platform for our next generation researchers to establish their social networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Important Dates&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This session will be held on &lt;strong&gt;2-3 Dec, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Contacts&lt;/h2&gt;
&lt;p&gt;For any questions regarding this session, please email the PhD School Coordinators:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mahsa Baktashmotlagh&lt;/strong&gt; (&lt;a href=&quot;mailto:m.baktashmotlagh@uq.edu.au&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;m.baktashmotlagh@uq.edu.au&lt;/a&gt;), The University of Queensland, Australia&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Zheng Zhang&lt;/strong&gt; (&lt;a href=&quot;mailto:darrenzz219@gmail.com&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;darrenzz219@gmail.com&lt;/a&gt;), Harbin Institute of Technology, Shenzhen, China&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tianran Hu&lt;/strong&gt; (&lt;a href=&quot;mailto:thu03@wm.edu&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;thu03@wm.edu&lt;/a&gt;), College of William &amp;#x26; Mary, USA&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Call for Grand Challenge Proposals]]></title><description><![CDATA[Overview ACM Multimedia Asia is calling for proposals for Grand Challenges in 2021. Proposers with an innovative idea of a Multimedia Grand…]]></description><link>https://www.acmmmasia.org/2021/call-for-grand-challenge-proposals/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/call-for-grand-challenge-proposals/</guid><pubDate>Wed, 14 Jul 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;ACM Multimedia Asia is calling for proposals for Grand Challenges in 2021. Proposers with an innovative idea of a Multimedia Grand Challenge, should gather an organisational team with the capacity to carry out the organisation of a challenge, and submit a proposal according to the instructions below.&lt;/p&gt;
&lt;p&gt;The purpose of the Multimedia Grand Challenge is to engage the multimedia research community by establishing well-defined and objectively judged challenge problems intended to exercise the state-of-the-art methods and inspire future research directions. The key criteria for Grand Challenges are that they should be useful, interesting, and their solution should involve a series of research tasks over a long period of time, with pointers towards longer-term research.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A Multimedia Grand Challenge proposal should include:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A brief description to explain why the challenge problem is important and relevant to the multimedia research community, industry, and society over the next 3-5 years or a longer horizon.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A description of a specific set of research tasks or sub-tasks to be carried out towards tackling the challenge problem in the long run.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An outline of current state-of-the-art techniques and why this Grand Challenge would help accelerate research in this important area.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Link to sites containing relevant datasets to be used for objective training and evaluation of the grand challenge tasks. Full appropriate documentation on the datasets should be provided or made accessible.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A description of rigorously defined objective criteria and/or procedures on how the submissions will be evaluated or judged.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A commitment to publish and maintain a website related to their specific Grand Challenge containing the information, datasets, tasks for the Grand Challenge at least the next 3 years.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Work with ACM Multimedia Asia Conference organisers to publicise the Grand Challenge tasks to researchers for participation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Contact information of at least two organisers who will be responsible for organizing, publicizing, reviewing and judging the Grand Challenge submissions as described in the proposal.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that although we ask organisers to express a multi-year commitment to their Grand Challenge, the Challenge will still undergo a new review each year. Priority will be given to Grand Challenges which have been successful in the past and are clearly contributing to continuity.&lt;/p&gt;
&lt;h2&gt;Submission Website&lt;/h2&gt;
&lt;p&gt;The grand challenge proposals should be submitted through: &lt;a href=&quot;https://cmt3.research.microsoft.com/MMASIA2021/&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;https://cmt3.research.microsoft.com/MMASIA2021/&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Important Dates&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; The submission deadline is at 11:59 p.m. of the stated deadline date &lt;a href=&quot;https://www.timeanddate.com/time/zones/aoe&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Anywhere on Earth&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Submission deadline: &lt;strong&gt;5 July, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Notification of acceptance: &lt;strong&gt;18 July, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Contacts&lt;/h2&gt;
&lt;p&gt;For any questions regarding the submission, please email the Grand Challenge Chairs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Shin’ichi Satoh&lt;/strong&gt; (&lt;a href=&quot;satoh@nii.ac.jp&quot;&gt;satoh@nii.ac.jp&lt;/a&gt;), National Institute of Informatics, Japan&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jingkuan Song&lt;/strong&gt; (&lt;a href=&quot;jingkuan.song@gmail.com&quot;&gt;jingkuan.song@gmail.com&lt;/a&gt;), University of Electronic Science and Technology of China, China&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Call for Tutorials]]></title><description><![CDATA[Call for Tutorial Proposals We are soliciting proposals for tutorials to be held in conjunction with the ACM Multimedia Asia 2021.
The…]]></description><link>https://www.acmmmasia.org/2021/call-for-tutorials/</link><guid isPermaLink="false">https://www.acmmmasia.org/2021/call-for-tutorials/</guid><pubDate>Thu, 01 Jul 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Call for Tutorial Proposals&lt;/h2&gt;
&lt;p&gt;We are soliciting proposals for tutorials to be held in conjunction with the ACM Multimedia Asia 2021.
The tutorials should aim to give a comprehensive review of a specific topic related to multimedia.
The topic should be of sufficient relevance and importance to attract significant interest from the multimedia community.
Typical tutorial audiences consist of graduate students, researchers and practitioners from both academia and industry.
We invite proposals for half-day tutorials.&lt;/p&gt;
&lt;h2&gt;Proposal Format&lt;/h2&gt;
&lt;p&gt;Each tutorial proposal (maximum 4 pages, in PDF format) must include:&lt;/p&gt;
&lt;!-- need to use html syntax for lists with sublists --&gt;
&lt;ol class=&quot;list-bold&quot;&gt;
   &lt;li&gt;&lt;strong&gt;Title of the tutorial.&lt;/strong&gt;&lt;/li&gt;
   &lt;li&gt;&lt;strong&gt;Tutorial organisers (name, contact and short biography).&lt;/strong&gt;&lt;/li&gt;
   &lt;li&gt;&lt;strong&gt;Scope and topics of the tutorial.&lt;/strong&gt;&lt;/li&gt;
   &lt;li&gt;&lt;strong&gt;Rationale:&lt;/strong&gt;
      &lt;ul&gt;
         &lt;li&gt;Why the tutorial is related to ACM Multimedia Asia 2021?&lt;/li&gt;
         &lt;li&gt;Why the topic is important?&lt;/li&gt;
         &lt;li&gt;Why the tutorial may attract a reasonable number of attendees?&lt;/li&gt;
         &lt;li&gt;A brief biography for each organiser and panelist.&lt;/li&gt;
      &lt;/ul&gt;
   &lt;/li&gt;
   &lt;li&gt;&lt;strong&gt;Tutorial details:&lt;/strong&gt;
      &lt;ul&gt;
         &lt;li&gt;Tutorial title.&lt;/li&gt;
         &lt;li&gt;Organisers’ names, titles, affiliations, emails, and brief sketches.&lt;/li&gt;
         &lt;li&gt;Course description with topics to be covered, along with a brief outline or schedule.&lt;/li&gt;
         &lt;li&gt;Particular details that might be unique to the proposed tutorial.&lt;/li&gt;
         &lt;li&gt;Expected target audience, in terms of both composition and estimated number of attendees.&lt;/li&gt;
         &lt;li&gt;Description of links to any planned materials or resources to be distributed to attendees.&lt;/li&gt;
         &lt;li&gt;A description of how this proposal relates to tutorials at previous conferences.&lt;/li&gt;
         &lt;li&gt;Names of potential participants and invited speakers (if any).&lt;/li&gt;
         &lt;li&gt;Organisers are expected to be fully committed and physically/virtually present at the tutorial.&lt;/li&gt;    
      &lt;/ul&gt;
   &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Important Dates&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; The submission deadline is at 11:59 p.m. of the stated deadline date &lt;a href=&quot;https://www.timeanddate.com/time/zones/aoe&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;&lt;em&gt;Anywhere on Earth&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tutorial proposal submission: &lt;strong&gt;30 Aug, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Decision notification: &lt;strong&gt;15 Sep, 2021&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Contacts&lt;/h2&gt;
&lt;p&gt;For questions regarding the submission, you can email the tutorial chairs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Liang Zheng&lt;/strong&gt; (&lt;a href=&quot;mailto:liang.zheng@anu.edu.au&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;liang.zheng@anu.edu.au&lt;/a&gt;), Australian National University, Australia&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Xin Yu&lt;/strong&gt; (&lt;a href=&quot;mailto:xin.yu@uts.edu.au&quot; target=&quot;_blank&quot; rel=&quot;noreferrer&quot;&gt;xin.yu@uts.edu.au&lt;/a&gt;), University of Technology Sydney, Australia&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item></channel></rss>