{
    "componentChunkName": "component---src-templates-article-js",
    "path": "/call-for-workshop-papers/",
    "result": {"data":{"site":{"siteMetadata":{"title":"ACM Multimedia Asia 2021"}},"markdownRemark":{"id":"32b262a8-921e-54a9-a89d-c7d8e6739894","excerpt":"Workshop 1: Visual Tasks and Challenges under Low-quality Multimedia Data https://workshopcv.github.io/ Overview The field of computer vision has been a…","html":"<h2>Workshop 1: Visual Tasks and Challenges under Low-quality Multimedia Data</h2>\n<p><strong><a href=\"https://workshopcv.github.io/\" target=\"_blank\" rel=\"noreferrer\"><em>https://workshopcv.github.io/</em></a></strong></p>\n<h3>Overview</h3>\n<p>The field of computer vision has been a research hotspot, and early research focused on high-quality images or daytime scenes with better illumination. Existing vision techniques have achieved better results with an approximately accuracy rate of 96% with these conditions. In practice, nearly 90% of criminal activities occur in the night scenes with low quality, especially in major cases. The video data collected by the surveillance system in these scenes has low contrast and poor quality. According to the Ministry of Public Security Evidence Identification Center (China), the proportion of poor quality video images at night is as high as 95%, and the performance of current methods on low-quality visible images is low, which is difficult to cope with the actual security needs. There is an urgent need to optimise this problem.</p>\n<h3>Challenge</h3>\n<p>The goal of this challenge is to:</p>\n<ul>\n<li>Bring together the state of the art research on object detection under low illumination;</li>\n<li>Call for a coordinated effort to understand the opportunities and challenges emerging in object detection;</li>\n<li>Identify key tasks and evaluate the state-of-the-art methods;</li>\n<li>Showcase innovative methodologies and ideas;</li>\n<li>Introduce interesting real-world intelligent object detection under low illumination;</li>\n<li>Propose new real-world datasets and discuss future directions. We believe the workshop will offer a timely collection of research updates to benefit the researchers and practitioners working in the broad computer vision, multimedia, and pattern recognition communities.</li>\n</ul>\n<h3>Call for Papers</h3>\n<p>Except for the challenge, we solicit original research and survey papers in (but not limited to) the following topics:</p>\n<ol>\n<li>Pedestrian detection in low illumination, low resolution, rain and fog, etc.</li>\n<li>Object detection in low illumination, low resolution, rain and fog, etc.</li>\n<li>Person re-identification in low illumination, low resolution, rain and fog, etc.</li>\n<li>Object recognition in low illumination, low resolution, rain and fog, etc.</li>\n<li>Segmentation in low illumination, low resolution, rain and fog, etc.</li>\n<li>Counting in low illumination, low resolution, rain and fog, etc.</li>\n</ol>\n<!-- ### Important Dates\n\n-\tRelease of Training Date: **10 August, 2021**.\n-\tRelease of Validation Date:\t**10 September, 2021**.\n-\tRelease of Test Date: **24 September, 2021**.\n-\tResult Submission Close: **8 October, 2021**.\n-\tWorkshop Paper Submission: **18 October, 2021**.\n-\tWorkshop Notification: **1 November, 2021**. -->\n<h3>Organisers</h3>\n<ul>\n<li><strong>Jing Xiao</strong>, (<a href=\"mailto:jing@whu.edu.cn\" target=\"_blank\" rel=\"noreferrer\">jing@whu.edu.cn</a>), Wuhan University, China</li>\n<li><strong>Xiao Wang</strong>, (<a href=\"mailto:hebeiwangxiao@whu.edu.cn\" target=\"_blank\" rel=\"noreferrer\">hebeiwangxiao@whu.edu.cn</a>), Wuhan University, China</li>\n<li><strong>Liang Liao</strong>, (<a href=\"mailto:liang@nii.ac.jp\" target=\"_blank\" rel=\"noreferrer\">liang@nii.ac.jp</a>), National Institute of Informatics, Japan</li>\n<li><strong>Shin’ichi Satoh</strong>, (<a href=\"mailto:satoh@nii.ac.jp\" target=\"_blank\" rel=\"noreferrer\">satoh@nii.ac.jp</a>), National Institute of Informatics, Japan</li>\n<li><strong>Chia-wen Lin</strong>, (<a href=\"mailto:cwlin@ee.nthu.edu.tw\" target=\"_blank\" rel=\"noreferrer\">cwlin@ee.nthu.edu.tw</a>), National Tsing Hua University, Taiwan</li>\n</ul>\n<p> </p>\n<hr>\n<h2>Workshop 2: Multi-Modal Embedding and Understanding</h2>\n<p><strong><a href=\"https://mmeu.github.io/\" target=\"_blank\" rel=\"noreferrer\"><em>https://mmeu.github.io/</em></a></strong></p>\n<h3>Overview</h3>\n<p>We humans perceive the physical world via multiple ways, e.g., watching, touching, hearing, and so on, which means that we process multi-modal information for environment perception. Multi-modal understanding plays a crucial role in enabling the machine with such ability. Due to its research significance, multi-modal embedding and understanding has gained much research attention and achieved much progress in the past couple of years. The recent advances in deep learning inspire us to explore more and deeper for the multi-modal embedding and understanding, such as self-supervised learning and pre-training in it. In this workshop, we aim to bring together researchers from the field of multimedia to discuss recent research and future directions for multi-modal embedding and understanding, and their applications.</p>\n<h3>Call for Papers</h3>\n<p>Multi-modal understanding are important and fundamental problems in the field of multimodal analysis, which have been attracting much research attention in recent years. Previous works have explored shallow embedding and understanding in many downstream tasks, including cross-modal retrieval, visual navigation, VQA, visual captioning, etc. To encourage researchers to explore new and advanced techniques in this area, we are organizing a workshop on “multi-modal embedding and understanding” with the conjunction of ACM MM Asia 2021, and calling for contributions. The included (but not limited) topics are as follows:</p>\n<ol>\n<li>Large-scale pre-training for multi-modal embedding and understanding</li>\n<li>Self-supervised learning in multi-modal embedding and understanding</li>\n<li>Semi-supervised learning in multi-modal embedding and understanding</li>\n<li>Contrastive learning in multi-modal embedding and understanding</li>\n<li>Interpretability in multi-modal embedding and understanding</li>\n<li>Interactive multi-modal understanding</li>\n<li>Trust AI for multi-modal understanding</li>\n<li>Cross-modal matching and retrieval</li>\n<li>Cross-modal understanding</li>\n<li>Multi-modal deep fake generation and detection</li>\n<li>And other related…</li>\n</ol>\n<h3>Submission Guidelines</h3>\n<p><strong>Format:</strong> Submitted papers (.pdf format) must use the ACM Article Template <a href=\"https://www.acm.org/publications/proceedings-template\" target=\"_blank\" rel=\"noreferrer\"><em>https://www.acm.org/publications/proceedings-template</em></a>. Please remember to add Concepts and Keywords.</p>\n<p><strong>Length:</strong> Papers must be <strong>no longer than 6 pages</strong>, including all text and figures, and up to two additional pages may be added for references. The reference pages must only contain references. Over-length papers will be rejected without review.</p>\n<h3>Workshop Schedule</h3>\n<p><strong>Please note:</strong> The submission deadline is at 11:59 p.m. of the stated deadline date <a href=\"https://www.timeanddate.com/time/zones/aoe\" target=\"_blank\" rel=\"noreferrer\"><em>Anywhere on Earth</em></a>.</p>\n<ul>\n<li>Paper Submission Deadline: <strong>19 October, 2021</strong>.</li>\n<li>Notifications of Acceptance: <strong>1 November, 2021</strong>.</li>\n<li>Camera-ready Submission: <strong>7 November, 2021</strong>.</li>\n</ul>\n<!-- ### Important dates\n- Paper Submission Deadline: **13 October, 2021**.\n- Notifications of Acceptance: **3 November, 2021**.\n- Camera-ready Submission: **10 November, 2021**. -->\n<h3>Organisers</h3>\n<ul>\n<li><strong>Wenguan Wang</strong>, ETH Zurich, Switzerland</li>\n<li><strong>Xiaojun Chang</strong>, RMIT, Australia</li>\n<li><strong>Yanli Ji</strong>, University of Electronic Science and Technology of China, China</li>\n<li><strong>Yi Bin</strong>, University of Electronic Science and Technology of China, China</li>\n</ul>\n<p> </p>\n<hr>\n<h2>Workshop 3: Multi-Model Computing of Marine Big Data</h2>\n<p><strong><a href=\"https://riverw.github.io/web/MCMBD/index.html\" target=\"_blank\" rel=\"noreferrer\"><em>https://riverw.github.io/web/MCMBD/index.html</em></a></strong></p>\n<h3>Overview</h3>\n<p>Different from the traditional multimedia technology which mainly focuses on human life, it is a novel and challenging problem to study multimedia data analysis methods for marine big data. Compared with traditional multimedia data, marine big data has big differences in feature distribution, content understanding, applications, etc. This makes existing multimedia analysis methods in target detection and recognition, tracking and depth estimation and other tasks cannot be simply applied to ocean data analysis. The study of multimedia data analysis technology with marine big data can help humans understand the marine, realise the detection and protection of ocean resources intelligently, and provide important technical support for the protection of various rare ocean resources.</p>\n<h3>Call for Papers</h3>\n<p>Marine multimedia data analysis and retrieval techniques are essential for marine resource exploration and marine environment prediction and forecasting. The main analytical tasks based on the marine domain include detection, identification, retrieval, tracking, and prediction forecasting of marine environmental data such as weather, temperature, humidity, and rainfall. Detection and protection of marine resources can be intelligent through detection, identification and tracking technologies, which provides important technical support for the protection of various types of rare marine resources. Today, in order to better understand the ocean, humans are rapidly collecting a wide variety of marine multimedia big data. Therefore, in this workshop, we will present the recent advances of multimedia technology in marine big data. The main analytical tasks based on the marine domain include detection, identification, retrieval, tracking, and prediction forecasting of marine environmental data such as weather, temperature, humidity, and rainfall. Exploring multi-modal data provides important technical support for understanding the marine and protecting various rare marine resources. We believe that this workshop will facilitate a closer integration of multimedia content analysis technologies with applications in the marine field. we solicit original research and survey papers in (but not limited):</p>\n<ul>\n<li>Marine object detection</li>\n<li>Marine object re-identification</li>\n<li>Cross-modal hash retrieval in the marine area</li>\n<li>Fine-grained identification of marine organisms</li>\n<li>Artificial Intelligence for coastal environment evolution prediction</li>\n<li>Artificial Intelligence for optimisation of an ecological dynamic model</li>\n<li>Marine big data mining methods</li>\n</ul>\n<!-- ### Important Dates\n\n\n-   Submission Deadline: **15 October, 2021**.\n-   Notifications of Acceptance: **15 November, 2021**.\n-   Camera-ready Submission: **20 November, 2021**. -->\n<h3>Organisers</h3>\n<ul>\n<li><strong>Jie Nie</strong>, Ocean University of China, China</li>\n<li><strong>Lei Huang</strong>, Ocean University of China; Pilot National Laboratory for Marine Science and Technology (Qingdao)</li>\n<li><strong>An-An Liu</strong>, Tianjin University, China</li>\n<li><strong>Junbo Guo</strong>, State Key Laboratory of Communication Content Cognition, People’s Daily Online, China</li>\n<li><strong>Zhiqiang Wei</strong>, Ocean University of China; Pilot National Laboratory for Marine Science and Technology (Qingdao)</li>\n</ul>","frontmatter":{"title":"Call for Workshop Papers","datePublished":"2021-08-19","dateModified":"2021-10-12","dateModifiedFormatted":"12 October, 2021","description":"The important dates for Workshop, Multi-Modal Embedding and Understanding, have been updated."}}},"pageContext":{"id":"32b262a8-921e-54a9-a89d-c7d8e6739894","previousPostId":"10040fef-399e-5a6c-80ec-ee8c8f8a0fe8","nextPostId":"f81051f3-54ce-5769-b865-ff26c907ff99"}},
    "staticQueryHashes": ["4054665497"]}